{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMC DATA TRANSFORMS\n",
    "\n",
    "This notebook performs essential data transformations on `tblRawStateData.csv` to prepare for beta testing.\n",
    "It is built from analysis first performed in the apmc_data_explorer notebook, which builds this tool, which produces\n",
    "an artifact which is run through the APMC Data Explorer notebook again, which builds sections in this tool, and so on.\n",
    "Eventually there should probably be splits performed for modularity, but let's cross that bridge later.\n",
    "\n",
    "## Transformations Performed\n",
    "\n",
    "1. **Deprecated/Sparse Field Removal**\n",
    "2. **NULL/N-A String Replacement**\n",
    "3. **Whitespace Normalization**\n",
    "4. **Garbage Record Detection**\n",
    "5. **Parent-Child Relationship Repair**\n",
    "6. **SAME Keyword Record Splitting**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification for Garbage Record Criteria\n",
    "\n",
    "The garbage detection criteria emerge from our analysis of the WorldCovers data model and the practical realities of how the data was populated:\n",
    "\n",
    "### 1. Childless Records (`nRawStateDataID` not used as any other record's `nRawStateDataID_parent`)\n",
    "\n",
    "The `nRawStateDataID_parent` field implements a **self-referential hierarchy** allowing postmark variants to be grouped together (e.g., multiple size variants of the same basic design from the same town). Records that serve as parent anchors for other records have demonstrable structural importance. Conversely, records with no children that also have no content are likely scaffolding artifacts from import processes or abandoned data entry sessions.\n",
    "\n",
    "### 2. Orphaned Records (`nRawStateDataID_parent = 0` OR `nRawStateDataID_parent = nRawStateDataID`)\n",
    "\n",
    "Our data model analysis revealed three patterns in the parent relationship:\n",
    "- **Self-referencing** (parent = self): The typical pattern for standalone catalog entries — these are \"root\" records that are their own parent\n",
    "- **Zero/empty parent**: Records created without being assigned to a hierarchy — effectively orphaned\n",
    "- **True child records** (different parent): Records that are subordinate variants of another entry\n",
    "\n",
    "Records with zero parent or self-referencing that *also* lack content and approval represent incomplete data entry that was never finished or reviewed.\n",
    "\n",
    "### 3. Empty `txtRawStateData`\n",
    "\n",
    "The `txtRawStateData` field holds the **authoritative original catalog text** — the gold standard for each entry with ~97.8% population in working records. Our analysis established that \"the real value resides in well-parsed descriptive text fields from original catalog sources.\" A record without this core content has no scholarly value and represents either a placeholder that was never filled or a corrupted import.\n",
    "\n",
    "### 4. `approve_status != 'Approved'`\n",
    "\n",
    "The data model implements a **contributor/reviewer workflow** with statuses: 'Approved', 'Pending', 'Rejected', 'Deleted'. Records that haven't achieved 'Approved' status haven't passed quality review. Combined with the other criteria, unapproved records with no content or structural purpose are deferred work that never materialized.\n",
    "\n",
    "### Why ALL Criteria Must Be Met\n",
    "\n",
    "Each criterion alone would catch too many legitimate records:\n",
    "- Self-referencing is the *normal* pattern for standalone entries\n",
    "- Deleted records may still be referenced by images or historical analyses\n",
    "- Unapproved records may be valuable pending work\n",
    "\n",
    "Only records meeting **all four criteria simultaneously** are truly disposable — they are empty, abandoned, unreviewed, deleted shells with no structural dependencies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: ./wip/in/tblRawStateData_orig.csv\n",
      "Cleaned output: ./wip/out/tblRawStateData.csv\n",
      "Garbage records: ./wip/out/tblRawStateData_garbage.csv\n",
      "Duplicate records: ./wip/out/tblRawStateData_duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = './wip/in/tblRawStateData_orig.csv'\n",
    "OUTPUT_FILE = './wip/out/tblRawStateData.csv'\n",
    "GARBAGE_FILE = './wip/out/tblRawStateData_garbage.csv'\n",
    "DUPLICATES_FILE = './wip/out/tblRawStateData_duplicates.csv'\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Cleaned output: {OUTPUT_FILE}\")\n",
    "print(f\"Garbage records: {GARBAGE_FILE}\")\n",
    "print(f\"Duplicate records: {DUPLICATES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52,046 records with 62 columns\n",
      "\n",
      "Column names:\n",
      "['nRawStateDataID', 'nRawStateDataID_parent', 'nGroupOrder', 'nStateID', 'txtRawStateData', 'txtRawStateDataTemp', 'txtWorkingData', 'txtPostmark', 'txtDatesSeen', 'txtSizes', 'txtColors', 'txtRates', 'txtRatesText', 'txtValue', 'txtTerritory', 'txtTown', 'txtTownPostmark', 'txtTownmarkShape', 'txtTownmarkLettering', 'txtTownmarkDateFormat', 'txtTownmarkFraming', 'txtTownmarkRateLocation', 'txtTownmarkRateText', 'txtTownmarkRateValue', 'txtTownmarkColor', 'nWidth', 'nHeight', 'txtOther', 'ynEarliestKnownDate', 'ynLatestKnownDate', 'nEarliestUseDay', 'txtEarliestUseMonth', 'txtEarliestUseYear', 'nEarliestUseYear', 'nLatestUseDay', 'txtLatestUseMonth', 'nLatestUseYear', 'txtLatestUseYear', 'ynManuscript', 'ynBackstamp', 'txtPublishedID', 'txtDefaultImage', 'txtPDFPage', 'ynProcessed', 'memNotes', 'ynTownNameHasExtra', 'ynManuscriptTownmarks', 'nOrder', 'ynDeleted', 'dtEntered', 'dtUpdated', 'nImageCount', 'ynForReview', 'txtReasonForReview', 'txtMarkedBy', 'dtMarkedForReview', 'approve_status', 'request_status', 'txtUserEmail', 'ynEmailCheck', 'submitterId', 'approverId']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV with all columns as strings initially to preserve data\n",
    "df = pd.read_csv(INPUT_FILE, dtype=str, low_memory=False)\n",
    "\n",
    "print(f\"Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "print(f\"\\nColumn names:\\n{list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original record count: 52,046\n"
     ]
    }
   ],
   "source": [
    "# Store original counts for reporting\n",
    "original_count = len(df)\n",
    "print(f\"Original record count: {original_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation 1: Deprecated/Sparse Field Removal\n",
    "\n",
    "Based on a combination analysis of field population and application source code, we can remove the columns in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining columns:\n",
      "   0. nRawStateDataID\n",
      "   1. nRawStateDataID_parent\n",
      "   2. nStateID\n",
      "   3. txtRawStateData\n",
      "   4. txtRawStateDataTemp\n",
      "   5. txtPostmark\n",
      "   6. txtDatesSeen\n",
      "   7. txtSizes\n",
      "   8. txtColors\n",
      "   9. txtRates\n",
      "  10. txtRatesText\n",
      "  11. txtValue\n",
      "  12. txtTown\n",
      "  13. txtTownPostmark\n",
      "  14. txtTownmarkShape\n",
      "  15. txtTownmarkLettering\n",
      "  16. txtTownmarkDateFormat\n",
      "  17. txtTownmarkFraming\n",
      "  18. txtTownmarkColor\n",
      "  19. nWidth\n",
      "  20. nHeight\n",
      "  21. txtOther\n",
      "  22. nEarliestUseDay\n",
      "  23. nEarliestUseYear\n",
      "  24. nLatestUseDay\n",
      "  25. nLatestUseYear\n",
      "  26. ynManuscript\n",
      "  27. ynBackstamp\n",
      "  28. txtDefaultImage\n",
      "  29. nOrder\n",
      "  30. ynDeleted\n",
      "  31. nImageCount\n",
      "  32. ynForReview\n",
      "  33. approve_status\n"
     ]
    }
   ],
   "source": [
    "# Columns to remove\n",
    "COLUMNS_TO_DROP = [\n",
    "    'txtPublishedID',\n",
    "    'txtMarkedBy', \n",
    "    'txtPDFPage',\n",
    "    'memNotes',\n",
    "    'submitterId',\n",
    "    'approverId',\n",
    "    'dtMarkedForReview',\n",
    "    'txtTownmarkRateLocation',\n",
    "    'txtTownmarkRateValue',\n",
    "    'txtTownmarkRateText',\n",
    "    'txtReasonForReview',\n",
    "    'txtTerritory',\n",
    "    'ynEmailCheck',\n",
    "    'txtUserEmail',\n",
    "    'dtEntered',\n",
    "    'dtUpdated',\n",
    "    'nGroupOrder',\n",
    "    'txtWorkingData',\n",
    "    'ynLatestKnownDate',\n",
    "    'ynEarliestKnownDate',\n",
    "    'txtEarliestUseMonth',\n",
    "    'txtEarliestUseYear',\n",
    "    'txtLatestUseMonth',\n",
    "    'txtLatestUseYear',\n",
    "    'ynProcessed',\n",
    "    'ynTownNameHasExtra',\n",
    "    'ynManuscriptTownmarks',\n",
    "    'request_status',\n",
    "    'ynForReview'\n",
    "]\n",
    "\n",
    "# Verify columns exist before dropping\n",
    "existing_cols_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n",
    "missing_cols = [col for col in COLUMNS_TO_DROP if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: These columns not found in dataframe: {missing_cols}\")\n",
    "\n",
    "df = df.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "print(f\"\\nRemaining columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformation 2: Replace NULL, N/A, and '--' Strings with Actual Nulls\n",
    "\n",
    "The data contains widespread pollution from literal string values (case-insensitive) \"NULL\", \"n/a\", and \"--\" that should be actual null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL/N-A/-- strings found BEFORE replacement: 1,072\n",
      "\n",
      "Top 10 affected columns:\n",
      "  txtValue: 709\n",
      "  txtDatesSeen: 353\n",
      "  txtRatesText: 7\n",
      "  txtColors: 2\n",
      "  txtSizes: 1\n"
     ]
    }
   ],
   "source": [
    "def count_null_strings(dataframe, patterns):\n",
    "    \"\"\"Count occurrences of NULL-like strings across all columns.\"\"\"\n",
    "    counts = {}\n",
    "    for col in dataframe.columns:\n",
    "        col_series = dataframe[col].astype(str).str.strip().str.lower()\n",
    "        for pattern in patterns:\n",
    "            mask = col_series == pattern.lower()\n",
    "            count = mask.sum()\n",
    "            if count > 0:\n",
    "                if col not in counts:\n",
    "                    counts[col] = 0\n",
    "                counts[col] += count\n",
    "    return counts\n",
    "\n",
    "# Patterns to replace (case-insensitive)\n",
    "null_patterns = ['NULL', 'n/a', 'N/A', 'na', 'NA', '--']\n",
    "\n",
    "# Count before replacement\n",
    "before_counts = count_null_strings(df, null_patterns)\n",
    "total_before = sum(before_counts.values())\n",
    "\n",
    "print(f\"NULL/N-A/-- strings found BEFORE replacement: {total_before:,}\")\n",
    "print(f\"\\nTop 10 affected columns:\")\n",
    "for col, count in sorted(before_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {col}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL/N-A strings AFTER replacement: 0\n",
      "Strings converted to null: 1,072\n"
     ]
    }
   ],
   "source": [
    "def replace_null_strings(dataframe, patterns):\n",
    "    \"\"\"Replace NULL-like string patterns with actual NaN (null) values.\"\"\"\n",
    "    df_cleaned = dataframe.copy()\n",
    "    \n",
    "    # Create a case-insensitive pattern\n",
    "    pattern_regex = '|'.join([f'^{re.escape(p)}$' for p in patterns])\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        # Replace patterns with NaN (case-insensitive)\n",
    "        df_cleaned[col] = df_cleaned[col].replace(\n",
    "            to_replace=pattern_regex,\n",
    "            value=np.nan,\n",
    "            regex=True\n",
    "        )\n",
    "        # Also handle after stripping whitespace\n",
    "        mask = df_cleaned[col].astype(str).str.strip().str.lower().isin([p.lower() for p in patterns])\n",
    "        df_cleaned.loc[mask, col] = np.nan\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Apply the replacement\n",
    "df = replace_null_strings(df, null_patterns)\n",
    "\n",
    "# Verify replacement\n",
    "after_counts = count_null_strings(df, null_patterns)\n",
    "total_after = sum(after_counts.values()) if after_counts else 0\n",
    "\n",
    "print(f\"NULL/N-A strings AFTER replacement: {total_after:,}\")\n",
    "print(f\"Strings converted to null: {total_before - total_after:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformation 2: Normalize Whitespace\n",
    "\n",
    "Text fields may contain inconsistent whitespace that affects matching and display. This transformation:\n",
    "- Removes leading and trailing whitespace\n",
    "- Collapses multiple consecutive spaces into single spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace issues BEFORE normalization:\n",
      "  Leading/trailing whitespace: 87,529\n",
      "  Multiple inner spaces: 254\n"
     ]
    }
   ],
   "source": [
    "def count_whitespace_issues(dataframe):\n",
    "    \"\"\"Count whitespace issues across string columns.\"\"\"\n",
    "    issues = {'leading_trailing': 0, 'multiple_inner': 0}\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        col_series = dataframe[col].astype(str)\n",
    "        \n",
    "        # Leading/trailing whitespace\n",
    "        stripped = col_series.str.strip()\n",
    "        issues['leading_trailing'] += (col_series != stripped).sum()\n",
    "        \n",
    "        # Multiple inner spaces\n",
    "        has_multiple = col_series.str.contains(r'  +', regex=True, na=False)\n",
    "        issues['multiple_inner'] += has_multiple.sum()\n",
    "    \n",
    "    return issues\n",
    "\n",
    "ws_before = count_whitespace_issues(df)\n",
    "print(f\"Whitespace issues BEFORE normalization:\")\n",
    "print(f\"  Leading/trailing whitespace: {ws_before['leading_trailing']:,}\")\n",
    "print(f\"  Multiple inner spaces: {ws_before['multiple_inner']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace issues AFTER normalization:\n",
      "  Leading/trailing whitespace: 0\n",
      "  Multiple inner spaces: 0\n"
     ]
    }
   ],
   "source": [
    "def normalize_whitespace(dataframe):\n",
    "    \"\"\"Normalize whitespace in all string columns.\"\"\"\n",
    "    df_cleaned = dataframe.copy()\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        if df_cleaned[col].dtype == object:\n",
    "            # Convert to string first\n",
    "            df_cleaned[col] = df_cleaned[col].astype(str)\n",
    "            # Replace newlines/carriage returns with spaces  <-- ADD THIS\n",
    "            df_cleaned[col] = df_cleaned[col].str.replace(r'[\\r\\n]+', ' ', regex=True)\n",
    "            # Strip leading/trailing whitespace\n",
    "            df_cleaned[col] = df_cleaned[col].str.strip()\n",
    "            # Collapse multiple spaces into single space\n",
    "            df_cleaned[col] = df_cleaned[col].str.replace(r'  +', ' ', regex=True)\n",
    "            # Restore NaN for 'nan' strings created by astype\n",
    "            df_cleaned.loc[df_cleaned[col] == 'nan', col] = np.nan\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df = normalize_whitespace(df)\n",
    "\n",
    "ws_after = count_whitespace_issues(df)\n",
    "print(f\"Whitespace issues AFTER normalization:\")\n",
    "print(f\"  Leading/trailing whitespace: {ws_after['leading_trailing']:,}\")\n",
    "print(f\"  Multiple inner spaces: {ws_after['multiple_inner']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformation 3: Garbage Record Detection\n",
    "\n",
    "Identify records that meet ALL of the following criteria (indicating they are disposable):\n",
    "1. Childless (not referenced as parent by any other record)\n",
    "2. Orphaned (parent is 0, empty, or self-referencing)\n",
    "3. Empty content (txtRawStateData is null/empty)\n",
    "4. Not approved\n",
    "\n",
    "Note that I used to remove all records marked with ynDelete as 1, but somehow some legitimate records had been tagged, so I've removed that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique parent IDs referenced: 43,055\n",
      "1. Childless records: 8,992\n",
      "2. Orphaned records (parent=0/empty/self): 51,842\n",
      "3. Empty txtRawStateData: 1,119\n",
      "4. Not Approved: 535\n"
     ]
    }
   ],
   "source": [
    "# Find all IDs that are used as parents\n",
    "used_as_parent = set(df['nRawStateDataID_parent'].dropna().astype(str).unique())\n",
    "print(f\"Unique parent IDs referenced: {len(used_as_parent):,}\")\n",
    "\n",
    "# Criterion 1: Childless (not used as parent by other records)\n",
    "df['_is_childless'] = ~df['nRawStateDataID'].astype(str).isin(used_as_parent)\n",
    "print(f\"1. Childless records: {df['_is_childless'].sum():,}\")\n",
    "\n",
    "# Criterion 2: Orphaned (parent is 0, empty, or self)\n",
    "def is_orphaned(row):\n",
    "    parent = str(row['nRawStateDataID_parent']) if pd.notna(row['nRawStateDataID_parent']) else ''\n",
    "    self_id = str(row['nRawStateDataID']) if pd.notna(row['nRawStateDataID']) else ''\n",
    "    return parent == '' or parent == '0' or parent == self_id\n",
    "\n",
    "df['_is_orphaned'] = df.apply(is_orphaned, axis=1)\n",
    "print(f\"2. Orphaned records (parent=0/empty/self): {df['_is_orphaned'].sum():,}\")\n",
    "\n",
    "# Criterion 3: Empty txtRawStateData\n",
    "df['_empty_content'] = df['txtRawStateData'].isna() | (df['txtRawStateData'].astype(str).str.strip() == '')\n",
    "print(f\"3. Empty txtRawStateData: {df['_empty_content'].sum():,}\")\n",
    "\n",
    "# Criterion 4: Not Approved\n",
    "df['_not_approved'] = df['approve_status'] != 'Approved'\n",
    "print(f\"4. Not Approved: {df['_not_approved'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GARBAGE DETECTION RESULTS ===\n",
      "Total records: 52,046\n",
      "Garbage records (ALL criteria met): 335 (0.64%)\n",
      "Clean records: 51,711 (99.36%)\n"
     ]
    }
   ],
   "source": [
    "# Combine all criteria - a record is garbage only if ALL conditions are true\n",
    "df['_is_garbage'] = (\n",
    "    df['_is_childless'] &\n",
    "    df['_is_orphaned'] &\n",
    "    df['_empty_content'] &\n",
    "    df['_not_approved']\n",
    ")\n",
    "\n",
    "garbage_count = df['_is_garbage'].sum()\n",
    "clean_count = len(df) - garbage_count\n",
    "\n",
    "print(f\"\\n=== GARBAGE DETECTION RESULTS ===\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Garbage records (ALL criteria met): {garbage_count:,} ({garbage_count/len(df)*100:.2f}%)\")\n",
    "print(f\"Clean records: {clean_count:,} ({clean_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Garbage Records (first 10) ===\n",
      "      nRawStateDataID nRawStateDataID_parent txtRawStateData               txtTown approve_status ynDeleted\n",
      "51359           52362                      0             NaN              Fairview        Deleted         1\n",
      "51360           52363                      0             NaN      Berkeley Springs        Deleted         1\n",
      "51361           52364                      0             NaN               Bethany        Deleted         1\n",
      "51362           52365                      0             NaN  Blue Sulphur Springs        Deleted         1\n",
      "51363           52366                      0             NaN         Capon Springs        Deleted         1\n",
      "51366           52369                      0             NaN             Lexington        Deleted         1\n",
      "51452           52455                      0             NaN                DELETE        Deleted         1\n",
      "51453           52456                      0             NaN                DELETE        Deleted         1\n",
      "51455           52463                      0             NaN          Alabama Town        Pending         1\n",
      "51456           53457                      0             NaN                *Paris        Pending         1\n"
     ]
    }
   ],
   "source": [
    "# Show sample garbage records for verification\n",
    "if garbage_count > 0:\n",
    "    print(\"\\n=== Sample Garbage Records (first 10) ===\")\n",
    "    garbage_df = df[df['_is_garbage']]\n",
    "    display_cols = ['nRawStateDataID', 'nRawStateDataID_parent', 'txtRawStateData', \n",
    "                    'txtTown', 'approve_status', 'ynDeleted']\n",
    "    display_cols = [c for c in display_cols if c in garbage_df.columns]\n",
    "    print(garbage_df[display_cols].head(10).to_string())\n",
    "else:\n",
    "    print(\"\\nNo garbage records detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting records: 52,046\n",
      "After criterion 1 (childless): 8,992\n",
      "After criteria 1+2 (+ orphaned): 8,788\n",
      "After criteria 1+2+3 (+ empty content): 1,094\n",
      "After criteria 1+2+3+4 (+ not approved): 335\n"
     ]
    }
   ],
   "source": [
    "# Progressive criteria analysis - show how records get filtered at each step\n",
    "\n",
    "c1 = df['_is_childless']\n",
    "c2 = df['_is_orphaned']\n",
    "c3 = df['_empty_content']\n",
    "c4 = df['_not_approved']\n",
    "\n",
    "print(f\"Starting records: {len(df):,}\")\n",
    "print(f\"After criterion 1 (childless): {c1.sum():,}\")\n",
    "print(f\"After criteria 1+2 (+ orphaned): {(c1 & c2).sum():,}\")\n",
    "print(f\"After criteria 1+2+3 (+ empty content): {(c1 & c2 & c3).sum():,}\")\n",
    "print(f\"After criteria 1+2+3+4 (+ not approved): {(c1 & c2 & c3 & c4).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformation 4: Parent-Child Relationship Repair\n",
    "\n",
    "The `nRawStateDataID_parent` field is used to denote hierarchical relationships where a record's field values are derived from a previous record's `txtRawStateData`. Records requiring a parent are identified by `txtRawStateData` beginning (case-insensitive) with:\n",
    "\n",
    "- `Same` — Indicates a variant of the preceding postmark\n",
    "- `(L)` — Latest known use date reference\n",
    "- `*(L)` or `(L)*` — Latest known use with asterisk notation\n",
    "- `*(E)` or `(E)*` — Earliest known use with asterisk notation\n",
    "\n",
    "Since `nRawStateDataID` values were assigned in parent-then-child order during import, we can iterate through records sequentially to establish proper parentage.\n",
    "\n",
    "### Analysis Approach\n",
    "\n",
    "Before reassignment, we analyze the current state to identify:\n",
    "1. **Type A**: Records that SHOULD have a parent (based on text indicators) but currently DON'T\n",
    "2. **Type B**: Records that DO have a parent assigned but text DOESN'T indicate they should — these are anomalies requiring investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Current Parent-Child Relationship Status ===\n",
      "Records that SHOULD have a parent (text-based): 13,935\n",
      "Records that DO have a parent assigned: 204\n"
     ]
    }
   ],
   "source": [
    "# Define parent indicator patterns\n",
    "PARENT_INDICATOR_PATTERNS = [\n",
    "    r'^same',       # starts with \"same\" (case-insensitive)\n",
    "    r'^\\(L\\)',      # starts with \"(L)\"\n",
    "    r'^\\*\\(L\\)',    # starts with \"*(L)\"\n",
    "    r'^\\(L\\)\\*',    # starts with \"(L)*\"\n",
    "    r'^\\*\\(E\\)',    # starts with \"*(E)\"\n",
    "    r'^\\(E\\)\\*',    # starts with \"(E)*\"\n",
    "]\n",
    "COMBINED_INDICATOR_PATTERN = '|'.join(PARENT_INDICATOR_PATTERNS)\n",
    "\n",
    "def should_have_parent(txt):\n",
    "    \"\"\"Check if txtRawStateData indicates this record should have a parent.\"\"\"\n",
    "    if pd.isna(txt) or str(txt).strip() == '':\n",
    "        return False\n",
    "    return bool(re.match(COMBINED_INDICATOR_PATTERN, str(txt).strip(), re.IGNORECASE))\n",
    "\n",
    "def has_parent_assigned(row):\n",
    "    \"\"\"Check if nRawStateDataID_parent indicates a true parent (not self, not 0, not empty).\"\"\"\n",
    "    parent = row['nRawStateDataID_parent']\n",
    "    self_id = row['nRawStateDataID']\n",
    "    if pd.isna(parent) or str(parent).strip() == '' or str(parent) == '0':\n",
    "        return False\n",
    "    return str(parent) != str(self_id)\n",
    "\n",
    "# Apply checks to dataframe\n",
    "df['_should_have_parent'] = df['txtRawStateData'].apply(should_have_parent)\n",
    "df['_has_parent_assigned'] = df.apply(has_parent_assigned, axis=1)\n",
    "\n",
    "print(\"=== Current Parent-Child Relationship Status ===\")\n",
    "print(f\"Records that SHOULD have a parent (text-based): {df['_should_have_parent'].sum():,}\")\n",
    "print(f\"Records that DO have a parent assigned: {df['_has_parent_assigned'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type A (should have parent, doesn't): 13,854\n",
      "Type B (has parent, text doesn't indicate): 123\n"
     ]
    }
   ],
   "source": [
    "# Identify mismatches\n",
    "type_a = df[df['_should_have_parent'] & ~df['_has_parent_assigned']]\n",
    "type_b = df[~df['_should_have_parent'] & df['_has_parent_assigned']]\n",
    "\n",
    "print(f\"Type A (should have parent, doesn't): {len(type_a):,}\")\n",
    "print(f\"Type B (has parent, text doesn't indicate): {len(type_b):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type B Analysis: Duplicate Records from Botched Import\n",
    "\n",
    "Type B records are anomalies — they have a parent assigned but their `txtRawStateData` doesn't begin with any parent indicator. Investigation reveals these are **duplicate records from a failed import process**, not legitimate parent-child relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Distribution:\n",
      "nStateID\n",
      "46    90\n",
      "48    33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ID Range:\n",
      "   Min ID: 770\n",
      "   Max ID: 973\n",
      "\n",
      "Content Comparison with Parents:\n",
      "   Records with IDENTICAL txtRawStateData to parent: 123 of 123\n",
      "\n",
      "Approval Status:\n",
      "approve_status\n",
      "Approved    119\n",
      "Deleted       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if len(type_b) > 0:\n",
    "    # Check state distribution\n",
    "    print(f\"\\nState Distribution:\")\n",
    "    print(type_b['nStateID'].value_counts())\n",
    "    \n",
    "    # Uncomment to check creation date patterns in original data\n",
    "    #print(f\"\\nCreation Date Distribution (top 5):\")\n",
    "    #print(type_b['dtEntered'].value_counts().head())\n",
    "    \n",
    "    # Check ID ranges\n",
    "    type_b_ids = type_b['nRawStateDataID'].astype(int)\n",
    "    print(f\"\\nID Range:\")\n",
    "    print(f\"   Min ID: {type_b_ids.min()}\")\n",
    "    print(f\"   Max ID: {type_b_ids.max()}\")\n",
    "    \n",
    "    # Check if content matches parent\n",
    "    print(f\"\\nContent Comparison with Parents:\")\n",
    "    identical_count = 0\n",
    "    for _, row in type_b.iterrows():\n",
    "        parent_id = row['nRawStateDataID_parent']\n",
    "        child_txt = str(row['txtRawStateData']).strip() if pd.notna(row['txtRawStateData']) else ''\n",
    "        parent_row = df[df['nRawStateDataID'] == parent_id]\n",
    "        if len(parent_row) > 0:\n",
    "            parent_txt = str(parent_row.iloc[0]['txtRawStateData']).strip() if pd.notna(parent_row.iloc[0]['txtRawStateData']) else ''\n",
    "            if child_txt == parent_txt:\n",
    "                identical_count += 1\n",
    "    print(f\"   Records with IDENTICAL txtRawStateData to parent: {identical_count} of {len(type_b)}\")\n",
    "    \n",
    "    # Check approve_status\n",
    "    print(f\"\\nApproval Status:\")\n",
    "    print(type_b['approve_status'].value_counts())\n",
    "else:\n",
    "    print(\"No Type B records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These records have parent assignments but text doesn't indicate parentage.\n",
      "Investigation shows they are DUPLICATES with identical content to their 'parents'.\n",
      "\n",
      "ID: 770 | Parent: 8 | Town: nan\n",
      "   txtRawStateData: FREDERICKSBURG(“F” 5mm high, used as bkstp)(March 1, 1775;SL-50x3,MDD ...\n",
      "\n",
      "ID: 771 | Parent: 8 | Town: Fredericksburg\n",
      "   txtRawStateData: FREDERICKSBURG(“F” 5mm high, used as bkstp)(March 1, 1775;SL-50x3,MDD ...\n",
      "\n",
      "ID: 777 | Parent: 115 | Town: ALDIE\n",
      "   txtRawStateData: ALDIE/Va.(1851-53;33;PAID,5;Black,Blue,Brown,Red) 20...\n",
      "\n",
      "ID: 778 | Parent: 115 | Town: ALDIE\n",
      "   txtRawStateData: ALDIE/Va.(1851-53;33;PAID,5;Black,Blue,Brown,Red) 20...\n",
      "\n",
      "ID: 779 | Parent: 115 | Town: ALDIE\n",
      "   txtRawStateData: ALDIE/Va.(1851-53;33;PAID,5;Black,Blue,Brown,Red) 20...\n",
      "\n",
      "ID: 783 | Parent: 121 | Town: Alexandria\n",
      "   txtRawStateData: ALEXANDRIA/Va.(1848-54;32;5[box],5,10;Black,Red) 15...\n",
      "\n",
      "ID: 784 | Parent: 138 | Town: AMHERST C.H\n",
      "   txtRawStateData: AMHERST C.H./Va.(1838-46;30;PAID,5[C];Black,Red) 25...\n",
      "\n",
      "ID: 785 | Parent: 139 | Town: AYLETTS\n",
      "   txtRawStateData: AYLETTS/Va.(1841-51;30;PAID,FREE,5;Black,Blue,Red) 25...\n",
      "\n",
      "ID: 786 | Parent: 139 | Town: AYLETTS\n",
      "   txtRawStateData: AYLETTS/Va.(1841-51;30;PAID,FREE,5;Black,Blue,Red) 25...\n",
      "\n",
      "ID: 787 | Parent: 140 | Town: BALCONY FALLS\n",
      "   txtRawStateData: BALCONY FALLS/Va.(1850s;33;PAID;Black,Blue) 25...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample Type B records\n",
    "if len(type_b) > 0:\n",
    "    print(\"These records have parent assignments but text doesn't indicate parentage.\")\n",
    "    print(\"Investigation shows they are DUPLICATES with identical content to their 'parents'.\\n\")\n",
    "    \n",
    "    for _, row in type_b.head(10).iterrows():\n",
    "        record_id = row['nRawStateDataID']\n",
    "        parent_id = row['nRawStateDataID_parent']\n",
    "        txt = str(row['txtRawStateData'])[:70] if pd.notna(row['txtRawStateData']) else 'NULL'\n",
    "        print(f\"ID: {record_id} | Parent: {parent_id} | Town: {row['txtTown']}\")\n",
    "        print(f\"   txtRawStateData: {txt}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type B Diagnosis\n",
    "\n",
    "Based on investigation, Type B records exhibit these characteristics:\n",
    "\n",
    "| Finding | Detail |\n",
    "|---------|--------|\n",
    "| **States affected** | Only Virginia and West Virginia |\n",
    "| **Content** | ALL have **identical** `txtRawStateData` to their assigned parents |\n",
    "| **Creation date** | Most created in a single batch (April 19, 2022) |\n",
    "| **Bogus dates** | Some have impossible \"1905\" dates |\n",
    "| **ID clustering** | IDs clustered in a specific range |\n",
    "\n",
    "**Conclusion**: These are **duplicate records from a botched import process**, not legitimate parent-child relationships. They should be:\n",
    "1. Flagged as duplicates\n",
    "2. Excluded from the parentage reassignment process\n",
    "3. Saved separately for review/deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate records identified (Type B): 123\n"
     ]
    }
   ],
   "source": [
    "# Mark Type B records as duplicates\n",
    "df['_is_duplicate'] = ~df['_should_have_parent'] & df['_has_parent_assigned']\n",
    "duplicate_count = df['_is_duplicate'].sum()\n",
    "\n",
    "print(f\"Duplicate records identified (Type B): {duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent-Child Reassignment\n",
    "\n",
    "Now we perform the actual parentage reassignment for Type A records (those that should have a parent but don't). The algorithm:\n",
    "\n",
    "1. Sort records by `nRawStateDataID` (ascending) to process in import order\n",
    "2. Track the current \"parent context\" as we iterate\n",
    "3. When a record's text begins with a parent indicator, assign it the current parent\n",
    "4. When a record's text does NOT begin with a parent indicator, it becomes the new parent context\n",
    "5. Skip duplicate records (Type B) — they retain their existing (incorrect) parent assignment for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records to process: 52,046\n",
      "Duplicates to skip: 123\n"
     ]
    }
   ],
   "source": [
    "def reassign_parents(dataframe):\n",
    "    \"\"\"\n",
    "    Reassign nRawStateDataID_parent based on txtRawStateData indicators.\n",
    "    \n",
    "    Returns a copy of the dataframe with updated parent assignments and \n",
    "    a count of changes made.\n",
    "    \"\"\"\n",
    "    df_work = dataframe.copy()\n",
    "    \n",
    "    # Sort by ID to ensure proper ordering\n",
    "    df_work = df_work.sort_values('nRawStateDataID', key=lambda x: x.astype(int))\n",
    "    df_work = df_work.reset_index(drop=True)\n",
    "    \n",
    "    # Track changes\n",
    "    changes = 0\n",
    "    \n",
    "    # Current parent context (the most recent \"root\" record)\n",
    "    current_parent_id = None\n",
    "    \n",
    "    # Store new parent values\n",
    "    new_parents = []\n",
    "    \n",
    "    for idx, row in df_work.iterrows():\n",
    "        record_id = row['nRawStateDataID']\n",
    "        old_parent = row['nRawStateDataID_parent']\n",
    "        txt = str(row['txtRawStateData']).strip() if pd.notna(row['txtRawStateData']) else ''\n",
    "        is_duplicate = row['_is_duplicate'] if '_is_duplicate' in row else False\n",
    "        \n",
    "        # Skip duplicates - leave their parent as-is\n",
    "        if is_duplicate:\n",
    "            new_parents.append(old_parent)\n",
    "            continue\n",
    "        \n",
    "        # Check if this record should have a parent\n",
    "        needs_parent = should_have_parent(txt)\n",
    "        \n",
    "        if needs_parent and current_parent_id is not None:\n",
    "            # This is a child record - assign current parent\n",
    "            new_parent = current_parent_id\n",
    "            if str(old_parent) != str(new_parent):\n",
    "                changes += 1\n",
    "            new_parents.append(new_parent)\n",
    "        else:\n",
    "            # This is a root record - becomes new parent context\n",
    "            current_parent_id = record_id\n",
    "            # Root records are their own parent (self-referencing)\n",
    "            new_parent = record_id\n",
    "            if str(old_parent) != str(new_parent):\n",
    "                changes += 1\n",
    "            new_parents.append(new_parent)\n",
    "    \n",
    "    # Apply new parent values\n",
    "    df_work['nRawStateDataID_parent'] = new_parents\n",
    "    \n",
    "    return df_work, changes\n",
    "\n",
    "print(f\"Records to process: {len(df):,}\")\n",
    "print(f\"Duplicates to skip: {duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parent values changed: 19,039\n"
     ]
    }
   ],
   "source": [
    "# Perform the reassignment\n",
    "df, parent_changes = reassign_parents(df)\n",
    "\n",
    "print(f\"Total parent values changed: {parent_changes:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records that SHOULD have parent: 13,935\n",
      "Records that DO have parent: 13,935\n",
      "Remaining Type A mismatches: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify the reassignment\n",
    "df['_should_have_parent'] = df['txtRawStateData'].apply(should_have_parent)\n",
    "df['_has_parent_assigned'] = df.apply(has_parent_assigned, axis=1)\n",
    "\n",
    "# Exclude duplicates from verification\n",
    "df_non_dup = df[~df['_is_duplicate']]\n",
    "\n",
    "type_a_after = df_non_dup[df_non_dup['_should_have_parent'] & ~df_non_dup['_has_parent_assigned']]\n",
    "\n",
    "print(f\"Records that SHOULD have parent: {df_non_dup['_should_have_parent'].sum():,}\")\n",
    "print(f\"Records that DO have parent: {df_non_dup['_has_parent_assigned'].sum():,}\")\n",
    "print(f\"Remaining Type A mismatches: {len(type_a_after):,}\")\n",
    "\n",
    "if len(type_a_after) > 0:\n",
    "    print(f\"\\nNote: {len(type_a_after)} records still lack parents. These may be:\")\n",
    "    print(\"  - First record in a sequence (no prior parent available)\")\n",
    "    print(\"  - Records with indicators but no valid parent context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent 1: Alexa.(Alexandria)(E)(May 21, 1772;Ms;Black) 1,500...\n",
      "  └─ Child 2: (L)(Sept. 15, 1774) 1,000...\n",
      "\n",
      "Parent 8: FREDERICKSBURG(“F” 5mm high, used as bkstp)(March 1, 1775;SL...\n",
      "  └─ Child 9: (L)(June 27, 1775) 1,000...\n",
      "\n",
      "Parent 15: Norf(Norfolk)(E)(Aug. 21, 1765;Ms;Black) 1,500...\n",
      "  └─ Child 16: (L)(Dec. 6, 1765) 1,200...\n",
      "\n",
      "Parent 17: Nfk(Norfolk)(Nov. 20, 1772;Ms;Red) 1,000...\n",
      "  └─ Child 18: Same(--, 1773;Ms;Black) 1,000...\n",
      "\n",
      "Parent 19: NORFOLK(backstamp)(E)(Feb. 11, 1775;SL-29x5,MDD below;Black)...\n",
      "  └─ Child 20: (L)(Oct. 2, 1775) 1,200...\n",
      "\n",
      "Parent 19: NORFOLK(backstamp)(E)(Feb. 11, 1775;SL-29x5,MDD below;Black)...\n",
      "  └─ Child 21: Same(May 6, 1775;MDD same line) 1,500...\n",
      "\n",
      "Parent 19: NORFOLK(backstamp)(E)(Feb. 11, 1775;SL-29x5,MDD below;Black)...\n",
      "  └─ Child 22: Same(July 25, 1775;MDD below;Red) 1,500...\n",
      "\n",
      "Parent 26: SUFFOLK(April 12, 1775;SL-30x5,MDD below;Red) 1500...\n",
      "  └─ Child 27: Same(July 25, 1775;Black) 1,200...\n",
      "\n",
      "Parent 28: WBurg(Williamsburg)(E)(Nov. 14, 1734;Ms;Black) 2,500...\n",
      "  └─ Child 29: Same(--, 1745;Way[ms]) See Way Mail section 2,000...\n",
      "\n",
      "Parent 28: WBurg(Williamsburg)(E)(Nov. 14, 1734;Ms;Black) 2,500...\n",
      "  └─ Child 30: (L)(--, 1757) 2,000...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample of reassigned relationships\n",
    "children = df[(df['_should_have_parent']) & (df['_has_parent_assigned']) & (~df['_is_duplicate'])]\n",
    "\n",
    "for _, child in children.head(10).iterrows():\n",
    "    child_id = child['nRawStateDataID']\n",
    "    parent_id = child['nRawStateDataID_parent']\n",
    "    child_txt = str(child['txtRawStateData'])[:60] if pd.notna(child['txtRawStateData']) else 'NULL'\n",
    "    \n",
    "    # Get parent record\n",
    "    parent = df[df['nRawStateDataID'] == parent_id]\n",
    "    if len(parent) > 0:\n",
    "        parent_txt = str(parent.iloc[0]['txtRawStateData'])[:60] if pd.notna(parent.iloc[0]['txtRawStateData']) else 'NULL'\n",
    "        print(f\"Parent {parent_id}: {parent_txt}...\")\n",
    "        print(f\"  └─ Child {child_id}: {child_txt}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformation 5: Family Text Rollup (txtRawStateDataTemp)\n",
    "\n",
    "After parent-child relationships are established, we create a \"rolled up\" view of each catalog entry family. The `txtRawStateDataTemp` field is populated with an **uppercase concatenation** of:\n",
    "\n",
    "1. The parent record's `txtRawStateData`\n",
    "2. All child records' `txtRawStateData` (sorted by `nRawStateDataID`)\n",
    "\n",
    "This provides a complete, searchable view of the entire catalog entry including all variants.\n",
    "\n",
    "### Example\n",
    "**Before rollup:**\n",
    "- Parent (ID 1): `Alexa.(Alexandria)(E)(May 21, 1772;Ms;Black) 1,500`\n",
    "- Child (ID 2): `(L)(Sept. 15, 1774) 1,000`\n",
    "\n",
    "**After rollup (txtRawStateDataTemp for both records):**\n",
    "`ALEXA.(ALEXANDRIA)(E)(MAY 21, 1772;MS;BLACK) 1,500  (L)(SEPT. 15, 1774) 1,000`\n",
    "\n",
    "Records without children (standalone entries) receive their own `txtRawStateData` uppercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_family_rollup(dataframe, delimiter=' | '):\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Pass 1: clean text (only from the source column)\n",
    "    df['_txt_clean'] = (\n",
    "        df['txtRawStateData']\n",
    "          .fillna('')\n",
    "          .astype(str)\n",
    "          .replace(['nan', 'None', 'null', 'NaN'], '')\n",
    "          .str.strip()\n",
    "    )\n",
    "\n",
    "    # Build stable numeric IDs (avoids '1' vs '1.0' vs 'nan' string issues)\n",
    "    id_num = pd.to_numeric(df['nRawStateDataID'], errors='coerce')\n",
    "    parent_num = pd.to_numeric(df['nRawStateDataID_parent'], errors='coerce')\n",
    "\n",
    "    # Stable family id:\n",
    "    # - if parent exists => family is that parent id\n",
    "    # - else => family is the row's own id (root)\n",
    "    df['_family_id'] = parent_num.where(parent_num.notna(), id_num)\n",
    "\n",
    "    # If both ID and parent are missing, isolate by row index\n",
    "    # IMPORTANT: must be a Series, not an Index\n",
    "    fallback = df.index.to_series().astype('int64')\n",
    "    df['_family_id'] = df['_family_id'].fillna(fallback)\n",
    "\n",
    "    # Parent-first ordering inside each family:\n",
    "    # parent row is the one whose nRawStateDataID == _family_id\n",
    "    df['_is_parent'] = (id_num == df['_family_id']).fillna(False)\n",
    "\n",
    "    # Sort so parent is first, then by ID for stable ordering\n",
    "    df_sorted = df.sort_values(\n",
    "        by=['_family_id', '_is_parent', 'nRawStateDataID'],\n",
    "        ascending=[True, False, True]\n",
    "    )\n",
    "\n",
    "    # Pass 2: compute one rollup per family, then broadcast via map\n",
    "    def rollup_text(series):\n",
    "        vals = [v for v in series if v]\n",
    "        if not vals:\n",
    "            return \"\"\n",
    "\n",
    "        # If you do NOT want de-duping, remove this block.\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for v in vals:\n",
    "            if v not in seen:\n",
    "                seen.add(v)\n",
    "                uniq.append(v)\n",
    "\n",
    "        return delimiter.join(uniq).upper()\n",
    "\n",
    "    rollups = (\n",
    "        df_sorted\n",
    "        .groupby('_family_id', sort=False)['_txt_clean']\n",
    "        .apply(rollup_text)\n",
    "    )\n",
    "\n",
    "    df['txtRawStateDataTemp'] = df['_family_id'].map(rollups)\n",
    "\n",
    "    # Metrics\n",
    "    valid = df['txtRawStateDataTemp'] != \"\"\n",
    "    family_count = df.loc[valid, '_family_id'].nunique()\n",
    "    record_count = int(valid.sum())\n",
    "\n",
    "    print(\"Family rollup complete:\")\n",
    "    print(f\"  Families processed: {family_count:,}\")\n",
    "    print(f\"  Records updated: {record_count:,}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family rollup complete:\n",
      "  Families processed: 36,872\n",
      "  Records updated: 50,930\n"
     ]
    }
   ],
   "source": [
    "# Perform the family text rollup\n",
    "df = create_family_rollup(df, delimiter='  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on rollup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with txtRawStateDataTemp populated: 50,930 (97.9%)\n",
      "\n",
      "Family size distribution:\n",
      "  Single records (no children): 30,793\n",
      "  2 records (parent + 1 child): 4,440\n",
      "  3+ records: 2,755\n",
      "  Largest family: 82 records\n"
     ]
    }
   ],
   "source": [
    "# Check txtRawStateDataTemp population\n",
    "temp_populated = df['txtRawStateDataTemp'].notna() & (df['txtRawStateDataTemp'] != '')\n",
    "print(f\"Records with txtRawStateDataTemp populated: {temp_populated.sum():,} ({temp_populated.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Count families by size\n",
    "family_sizes = df.groupby('nRawStateDataID_parent').size()\n",
    "print(f\"\\nFamily size distribution:\")\n",
    "print(f\"  Single records (no children): {(family_sizes == 1).sum():,}\")\n",
    "print(f\"  2 records (parent + 1 child): {(family_sizes == 2).sum():,}\")\n",
    "print(f\"  3+ records: {(family_sizes >= 3).sum():,}\")\n",
    "print(f\"  Largest family: {family_sizes.max()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 51,392 clean records to: ./wip/out/tblRawStateData.csv\n",
      "Saved 531 garbage records to: ./wip/out/tblRawStateData_garbage.csv\n",
      "Saved 123 duplicate records to: ./wip/out/tblRawStateData_duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "# Separate records by category\n",
    "# Duplicates: Type B records from botched import (special case)\n",
    "duplicate_records = df[df['_is_duplicate']]\n",
    "\n",
    "# Clean: Approved AND not deleted AND not duplicate\n",
    "clean_mask = (\n",
    "    (df['approve_status'] == 'Approved') & \n",
    "    (~df['_is_duplicate'])\n",
    ")\n",
    "clean_records = df[clean_mask]\n",
    "\n",
    "# Garbage: Everything else (not clean AND not duplicate)\n",
    "garbage_records = df[~clean_mask & ~df['_is_duplicate']]\n",
    "\n",
    "# Uncomment to drop temporary/work fields for final output\n",
    "#\"\"\"\n",
    "temp_cols = [ '_is_childless',\n",
    "              '_is_orphaned',\n",
    "              '_empty_content',\n",
    "              '_not_approved',\n",
    "              '_should_have_parent',\n",
    "              '_has_parent_assigned',\n",
    "              '_is_duplicate',\n",
    "              '_is_garbage',\n",
    "              '_txt_clean',\n",
    "              '_family_id',\n",
    "              '_is_parent',\n",
    "              'txtRawStateDataTemp',\n",
    "              'approve_status',\n",
    "              'ynDeleted' ]\n",
    "clean_records = clean_records.drop(columns=temp_cols)\n",
    "duplicate_records = duplicate_records.drop(columns=temp_cols)\n",
    "garbage_records = garbage_records.drop(columns=temp_cols)\n",
    "#\"\"\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save cleaned data\n",
    "clean_records.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved {len(clean_records):,} clean records to: {OUTPUT_FILE}\")\n",
    "\n",
    "# Save garbage records separately for review\n",
    "garbage_records.to_csv(GARBAGE_FILE, index=False)\n",
    "print(f\"Saved {len(garbage_records):,} garbage records to: {GARBAGE_FILE}\")\n",
    "\n",
    "# Save duplicate records separately for review\n",
    "duplicate_records.to_csv(DUPLICATES_FILE, index=False)\n",
    "print(f\"Saved {len(duplicate_records):,} duplicate records to: {DUPLICATES_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covers-zQZJ1Gor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
