{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMC DATA EXPLORER\n",
    "\n",
    "This notebook provides analytical approaches for exploring the `txtRawStateData` field and related parsed fields to help determine optimal database normalization strategy.\n",
    "\n",
    "> The real value resides in well-parsed descriptive text fields from original catalog sources rather than elaborate normalized structures that often remain unused.\n",
    "\n",
    "## Contents\n",
    "[0. Setup and Data Loading](#0-Setup-and-Data-Loading) - _Load CSV data and configure display settings_\n",
    "\n",
    "[1. Cardinality Analysis](#1-Cardinality-Analysis) - _Determine which fields justify lookup tables vs inline storage_\n",
    "\n",
    "[2. Text Pattern Mining](#2-Text-Pattern-Mining) - _Extract structural patterns from txtRawStateData to understand formatting consistency_\n",
    "\n",
    "[3. Field Population Analysis](#3-Field-Population-Analysis) - _Identify essential vs sparsely-populated fields by population rates_\n",
    "\n",
    "[4. Normalization Trade-off Analysis](#4-Normalization-Trade-off-Analysis) - _Evaluate cost/benefit of different normalization approaches_\n",
    "\n",
    "[5. Philatelic-Specific Analysis](#5-Philatelic-Specific-Analysis) - _Domain-specific explorations: town names, dates, colors, sizes_\n",
    "\n",
    "[6. Raw vs Parsed Field Comparison](#6-Raw-vs-Parsed-Field-Comparison) - _Examine whether parsed fields capture all information from raw source_\n",
    "\n",
    "7\\. Specific Issue Investigation:\n",
    "   - [7.1. Geographic Coverage Analysis](#71-Geographic-Coverage-Analysis) - _State-by-state record distribution_\n",
    "   - [7.2. Color Handling Ambiguity Analysis](#72-Color-Handling-Ambiguity-Analysis) - _Investigate multi-color notation semantics and ASCC documentation gaps_\n",
    "\n",
    "[8. Decision Framework & Analysis Summary](#8-Decision-Framework--Analysis-Summary) - _Normalization criteria and final recommendations_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 51,392 records with 31 columns\n"
     ]
    }
   ],
   "source": [
    "# Load the main data file\n",
    "# Adjust path as needed for your environment\n",
    "CSV_PATH = \"./wip/out/tblRawStateData.csv\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print(f\"Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "\n",
    "# Uncomment to filter only to approved, non-deleted records\n",
    "#approved = df[(df['ynDeleted'] == 0) & (df['approve_status'] == 'Approved')].copy()\n",
    "#print(f\"Approved, non-deleted records: {len(approved):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset:\n",
      "   0. nRawStateDataID\n",
      "   1. nRawStateDataID_parent\n",
      "   2. nStateID\n",
      "   3. txtRawStateData\n",
      "   4. txtPostmark\n",
      "   5. txtDatesSeen\n",
      "   6. txtSizes\n",
      "   7. txtColors\n",
      "   8. txtRates\n",
      "   9. txtRatesText\n",
      "  10. txtValue\n",
      "  11. txtTown\n",
      "  12. txtTownPostmark\n",
      "  13. txtTownmarkShape\n",
      "  14. txtTownmarkLettering\n",
      "  15. txtTownmarkDateFormat\n",
      "  16. txtTownmarkFraming\n",
      "  17. txtTownmarkColor\n",
      "  18. nWidth\n",
      "  19. nHeight\n",
      "  20. txtOther\n",
      "  21. nEarliestUseDay\n",
      "  22. nEarliestUseYear\n",
      "  23. nLatestUseDay\n",
      "  24. nLatestUseYear\n",
      "  25. ynManuscript\n",
      "  26. ynBackstamp\n",
      "  27. txtDefaultImage\n",
      "  28. nOrder\n",
      "  29. nImageCount\n",
      "  30. ynForReview\n"
     ]
    }
   ],
   "source": [
    "# Quick overview of columns\n",
    "print(\"Columns in dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Cardinality Analysis\n",
    "\n",
    "Determine which fields justify lookup tables vs inline storage\n",
    "\n",
    "**Evidence for normalization (via lookup table)**:\n",
    "- Low cardinality (< 50-100 unique values)\n",
    "- Values appear repeatedly across records\n",
    "- Values need controlled vocabulary for filtering/faceting\n",
    "- Values need additional metadata (descriptions, display order)\n",
    "\n",
    "**Evidence _against_ normalization**:\n",
    "- High cardinality (approaches 1:1 with records)\n",
    "- Values are primarily free-text with minimal repetition\n",
    "- Forcing into categories loses information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality_analysis(df, columns_of_interest):\n",
    "    \"\"\"\n",
    "    Analyze fields to determine normalization candidates.\n",
    "    \n",
    "    Key metrics:\n",
    "    - Unique count vs total records\n",
    "    - Frequency distribution\n",
    "    - Top N values coverage percentage\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_records = len(df)\n",
    "    \n",
    "    for col in columns_of_interest:\n",
    "        non_null = df[df[col].notna() & (df[col] != '') & (df[col] != 'NULL')]\n",
    "        populated_count = len(non_null)\n",
    "        unique_values = non_null[col].nunique()\n",
    "        \n",
    "        if populated_count > 0:\n",
    "            # Top 10 values coverage\n",
    "            value_counts = non_null[col].value_counts()\n",
    "            top_10_coverage = value_counts.head(10).sum() / populated_count * 100\n",
    "            top_25_coverage = value_counts.head(25).sum() / populated_count * 100\n",
    "            \n",
    "            # Concentration ratio (does a small set dominate?)\n",
    "            concentration = value_counts.head(5).sum() / populated_count * 100\n",
    "            \n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'total_records': total_records,\n",
    "                'populated_count': populated_count,\n",
    "                'population_rate': populated_count / total_records * 100,\n",
    "                'unique_values': unique_values,\n",
    "                'cardinality_ratio': unique_values / populated_count,\n",
    "                'top_10_coverage_pct': top_10_coverage,\n",
    "                'top_20_coverage_pct': top_25_coverage,\n",
    "                'top_5_concentration_pct': concentration,\n",
    "                'normalization_candidate': unique_values < 100 and concentration > 50\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_value_distribution(df, column, top_n=30):\n",
    "    \"\"\"\n",
    "    Deep dive into a specific column's value distribution.\n",
    "    Useful for understanding if a controlled vocabulary fits.\n",
    "    \"\"\"\n",
    "    non_null = df[df[column].notna() & (df[column] != '') & (df[column] != 'NULL')]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Value Distribution: {column}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Populated: {len(non_null):,} ({len(non_null)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Unique values: {non_null[column].nunique():,}\")\n",
    "    print(f\"\\nTop {top_n} values:\")\n",
    "    print(non_null[column].value_counts().head(top_n).to_string())\n",
    "    \n",
    "    return non_null[column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>total_records</th>\n",
       "      <th>populated_count</th>\n",
       "      <th>population_rate</th>\n",
       "      <th>unique_values</th>\n",
       "      <th>cardinality_ratio</th>\n",
       "      <th>top_10_coverage_pct</th>\n",
       "      <th>top_20_coverage_pct</th>\n",
       "      <th>top_5_concentration_pct</th>\n",
       "      <th>normalization_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txtTownmarkShape</td>\n",
       "      <td>43068</td>\n",
       "      <td>1227</td>\n",
       "      <td>2.848983</td>\n",
       "      <td>24</td>\n",
       "      <td>0.019560</td>\n",
       "      <td>97.310513</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>92.094540</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txtTownmarkLettering</td>\n",
       "      <td>43068</td>\n",
       "      <td>3209</td>\n",
       "      <td>7.451008</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txtTownmarkDateFormat</td>\n",
       "      <td>43068</td>\n",
       "      <td>249</td>\n",
       "      <td>0.578155</td>\n",
       "      <td>10</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85.140562</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txtTownmarkFraming</td>\n",
       "      <td>43068</td>\n",
       "      <td>160</td>\n",
       "      <td>0.371506</td>\n",
       "      <td>6</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.375000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txtTownmarkColor</td>\n",
       "      <td>43068</td>\n",
       "      <td>3724</td>\n",
       "      <td>8.646791</td>\n",
       "      <td>93</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>95.649839</td>\n",
       "      <td>97.986037</td>\n",
       "      <td>89.661654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  column  total_records  populated_count  population_rate  \\\n",
       "0       txtTownmarkShape          43068             1227         2.848983   \n",
       "1   txtTownmarkLettering          43068             3209         7.451008   \n",
       "2  txtTownmarkDateFormat          43068              249         0.578155   \n",
       "3     txtTownmarkFraming          43068              160         0.371506   \n",
       "4       txtTownmarkColor          43068             3724         8.646791   \n",
       "\n",
       "   unique_values  cardinality_ratio  top_10_coverage_pct  top_20_coverage_pct  \\\n",
       "0             24           0.019560            97.310513           100.000000   \n",
       "1              4           0.001246           100.000000           100.000000   \n",
       "2             10           0.040161           100.000000           100.000000   \n",
       "3              6           0.037500           100.000000           100.000000   \n",
       "4             93           0.024973            95.649839            97.986037   \n",
       "\n",
       "   top_5_concentration_pct  normalization_candidate  \n",
       "0                92.094540                     True  \n",
       "1               100.000000                     True  \n",
       "2                85.140562                     True  \n",
       "3                99.375000                     True  \n",
       "4                89.661654                     True  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run cardinality analysis on classification fields\n",
    "classification_cols = [\n",
    "    'txtTownmarkShape', 'txtTownmarkLettering', 'txtTownmarkDateFormat',\n",
    "    'txtTownmarkFraming','txtTownmarkColor'\n",
    "]\n",
    "\n",
    "cardinality_df = cardinality_analysis(approved, classification_cols)\n",
    "cardinality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Value Distribution: txtTownmarkShape\n",
      "============================================================\n",
      "Total records: 43,068\n",
      "Populated: 1,227 (2.8%)\n",
      "Unique values: 24\n",
      "\n",
      "Top 30 values:\n",
      "txtTownmarkShape\n",
      "Straight line             484\n",
      "Circle                    381\n",
      "Double Circle             138\n",
      "Oval                       65\n",
      "Arc                        62\n",
      "Double Line Circle         18\n",
      "Double Oval                16\n",
      "Fancy/morticed             12\n",
      "Box                        10\n",
      "Octagon                     8\n",
      "Straight Line               5\n",
      "Pictoral                    4\n",
      "Dotted Oval                 4\n",
      "Double Line Oval            4\n",
      "Fancy Oval                  3\n",
      "Tombstone                   3\n",
      "Framed Arc                  2\n",
      "Fancy Box                   2\n",
      "Shell Design                1\n",
      "Dashed Circle               1\n",
      "Double Lined Box            1\n",
      "Dotted Circle               1\n",
      "Straight line - 2 line      1\n",
      "Straight line - 3 line      1\n",
      "\n",
      "============================================================\n",
      "Value Distribution: txtTownmarkLettering\n",
      "============================================================\n",
      "Total records: 43,068\n",
      "Populated: 3,209 (7.5%)\n",
      "Unique values: 4\n",
      "\n",
      "Top 30 values:\n",
      "txtTownmarkLettering\n",
      "Normal     3203\n",
      "Other         3\n",
      "Reverse       2\n",
      "Stencil       1\n",
      "\n",
      "============================================================\n",
      "Value Distribution: txtTownmarkDateFormat\n",
      "============================================================\n",
      "Total records: 43,068\n",
      "Populated: 249 (0.6%)\n",
      "Unique values: 10\n",
      "\n",
      "Top 30 values:\n",
      "txtTownmarkDateFormat\n",
      "MDD           70\n",
      "Manuscript    44\n",
      "Month-day     39\n",
      "YMDD          31\n",
      "MD            28\n",
      "YD            23\n",
      "MD or MDD      8\n",
      "YMD            3\n",
      "MDD or MD      2\n",
      "Quaker         1\n",
      "\n",
      "============================================================\n",
      "Value Distribution: txtTownmarkFraming\n",
      "============================================================\n",
      "Total records: 43,068\n",
      "Populated: 160 (0.4%)\n",
      "Unique values: 6\n",
      "\n",
      "Top 30 values:\n",
      "txtTownmarkFraming\n",
      "NOR               99\n",
      "Single line       53\n",
      "Double outer       4\n",
      "Other              2\n",
      "Pictoral/fancy     1\n",
      "Ornate             1\n",
      "\n",
      "============================================================\n",
      "Value Distribution: txtTownmarkColor\n",
      "============================================================\n",
      "Total records: 43,068\n",
      "Populated: 3,724 (8.6%)\n",
      "Unique values: 93\n",
      "\n",
      "Top 30 values:\n",
      "txtTownmarkColor\n",
      "Black                          2283\n",
      "Red                             512\n",
      "Blue                            280\n",
      "Black,Red                       169\n",
      "Green                            95\n",
      "Black,Blue                       62\n",
      "Black,Blue,Red                   57\n",
      "Blue,Red                         52\n",
      "Brown                            37\n",
      "Black,Brown                      15\n",
      "Red brown                        14\n",
      "Magenta                          10\n",
      "Brown,Red                         8\n",
      "Red orange                        8\n",
      "Orange                            7\n",
      "Red,Blue                          7\n",
      "Blue green                        5\n",
      "Black,Brown,Red                   4\n",
      "Black,Red,Blue                    4\n",
      "Black brown                       4\n",
      "n/a,Black                         4\n",
      "n/a,Red                           3\n",
      "Orange,Red                        3\n",
      "Black,Blue,Brown,Red              3\n",
      "Brownish red                      3\n",
      "Olive green                       2\n",
      "Blue,Orange,Red                   2\n",
      "Brown,Brownish,Brownish red       2\n",
      "Black,Blue,Green,Red              2\n",
      "Black brown,Red                   2\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into specific fields\n",
    "for col in cardinality_df[cardinality_df['normalization_candidate'] == True]['column']:\n",
    "    examine_value_distribution(approved, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Text Pattern Mining\n",
    "\n",
    "**Purpose**: Extract structural patterns from `txtRawStateData` to understand what information is present and how consistently it's formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_data_components(raw_text):\n",
    "    \"\"\"\n",
    "    Parse a txtRawStateData value into component parts.\n",
    "    \n",
    "    Typical format example:\n",
    "    - \"Alexa.(Alexandria)(E)(May 21, 1772;Ms;Black) 1,500\"\n",
    "    - \"FREDERICKSBURG(\\\"F\\\" 5mm high, used as bkstp)(March 1, 1775;SL-50x3,MDD below;Black,Red) 1,200\"\n",
    "    - \"(L)(June 27, 1775) 1,000\"\n",
    "    \n",
    "    Components:\n",
    "    - Town postmark text\n",
    "    - Town name (if different from postmark)\n",
    "    - (E) = Earliest known, (L) = Latest known\n",
    "    - Date range\n",
    "    - Size specifications (SL-50x3 = Straight Line 50mm x 3mm)\n",
    "    - Date format (MDD = Month-Day-Day)\n",
    "    - Colors\n",
    "    - Value\n",
    "    \"\"\"\n",
    "    if pd.isna(raw_text) or raw_text == 'NULL':\n",
    "        return None\n",
    "    \n",
    "    components = {\n",
    "        'raw': raw_text,\n",
    "        'has_earliest_marker': '(E)' in raw_text,\n",
    "        'has_latest_marker': '(L)' in raw_text,\n",
    "        'has_backstamp': 'backstamp' in raw_text.lower() or 'bkstp' in raw_text.lower(),\n",
    "        'has_manuscript': 'Ms' in raw_text,\n",
    "        'has_size_spec': bool(re.search(r'SL-\\d+x[\\d.]+', raw_text)),\n",
    "        'has_circle': bool(re.search(r'C-\\d+', raw_text)),\n",
    "        'has_value': bool(re.search(r'\\s[\\d,]+$', raw_text.strip())),\n",
    "    }\n",
    "    \n",
    "    # Extract size specifications\n",
    "    size_match = re.search(r'(SL|C)-(\\d+)x([\\d.]+)', raw_text)\n",
    "    if size_match:\n",
    "        components['shape'] = 'Straight line' if size_match.group(1) == 'SL' else 'Circle'\n",
    "        components['width'] = float(size_match.group(2))\n",
    "        components['height'] = float(size_match.group(3))\n",
    "    \n",
    "    # Extract colors (common patterns)\n",
    "    color_pattern = r';(Black|Red|Blue|Brown|Green|Orange|Magenta)(?:,|[)\\]])'\n",
    "    colors = re.findall(color_pattern, raw_text, re.IGNORECASE)\n",
    "    components['colors'] = colors\n",
    "    \n",
    "    # Extract value\n",
    "    value_match = re.search(r'\\s([\\d,]+)$', raw_text.strip())\n",
    "    if value_match:\n",
    "        components['value'] = value_match.group(1).replace(',', '')\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_frequency_analysis(df, column='txtRawStateData', sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze patterns in the raw data to understand formatting consistency.\n",
    "    This helps determine what parsing rules will work across the dataset.\n",
    "    \"\"\"\n",
    "    data = df[df[column].notna() & (df[column] != 'NULL')][column]\n",
    "    \n",
    "    if sample_size and len(data) > sample_size:\n",
    "        data = data.sample(sample_size, random_state=42)\n",
    "    \n",
    "    patterns = {\n",
    "        'has_parenthetical_name': 0,  # Town(Full Name)\n",
    "        'has_E_marker': 0,\n",
    "        'has_L_marker': 0,\n",
    "        'has_backstamp': 0,\n",
    "        'has_manuscript': 0,\n",
    "        'has_SL_size': 0,\n",
    "        'has_circle_size': 0,\n",
    "        'has_trailing_value': 0,\n",
    "        'has_color_spec': 0,\n",
    "        'has_date_format_spec': 0,  # MDD, MD, YMDD, etc.\n",
    "    }\n",
    "    \n",
    "    for text in data:\n",
    "        if re.search(r'\\([A-Z][a-z]+.*?\\)', text):\n",
    "            patterns['has_parenthetical_name'] += 1\n",
    "        if '(E)' in text:\n",
    "            patterns['has_E_marker'] += 1\n",
    "        if '(L)' in text:\n",
    "            patterns['has_L_marker'] += 1\n",
    "        if re.search(r'backstamp|bkstp', text, re.I):\n",
    "            patterns['has_backstamp'] += 1\n",
    "        if ';Ms;' in text or ';Ms,' in text:\n",
    "            patterns['has_manuscript'] += 1\n",
    "        if re.search(r'SL-\\d+', text):\n",
    "            patterns['has_SL_size'] += 1\n",
    "        if re.search(r'C-\\d+', text):\n",
    "            patterns['has_circle_size'] += 1\n",
    "        if re.search(r'\\s[\\d,]+$', text.strip()):\n",
    "            patterns['has_trailing_value'] += 1\n",
    "        if re.search(r';(Black|Red|Blue|Brown|Green)', text, re.I):\n",
    "            patterns['has_color_spec'] += 1\n",
    "        if re.search(r'MDD|MD |YMDD|YMD ', text):\n",
    "            patterns['has_date_format_spec'] += 1\n",
    "    \n",
    "    total = len(data)\n",
    "    print(f\"\\nPattern Frequency Analysis (n={total:,})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    for pattern, count in sorted(patterns.items(), key=lambda x: -x[1]):\n",
    "        pct = count/total*100\n",
    "        print(f\"{pattern:30} {count:6,} ({pct:5.1f}%)\")\n",
    "        results.append({'pattern': pattern, 'count': count, 'percentage': pct})\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pattern Frequency Analysis (n=5,000)\n",
      "==================================================\n",
      "has_trailing_value              4,608 ( 92.2%)\n",
      "has_color_spec                  2,632 ( 52.6%)\n",
      "has_parenthetical_name            698 ( 14.0%)\n",
      "has_manuscript                    287 (  5.7%)\n",
      "has_SL_size                       216 (  4.3%)\n",
      "has_circle_size                   210 (  4.2%)\n",
      "has_E_marker                      208 (  4.2%)\n",
      "has_L_marker                      179 (  3.6%)\n",
      "has_date_format_spec              151 (  3.0%)\n",
      "has_backstamp                       7 (  0.1%)\n"
     ]
    }
   ],
   "source": [
    "# Run pattern analysis on raw data\n",
    "pattern_df = pattern_frequency_analysis(approved, 'txtRawStateData', sample_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample txtRawStateData entries:\n",
      "======================================================================\n",
      " 1. West Batavia 1854 35\n",
      " 2. Same/VA.(1860s;32;Black) Union mail 20\n",
      " 3. Bethel 1841-42 10\n",
      " 4. Point Coupee 1821-50 75/40\n",
      " 5. Hudsonville 1841 75\n",
      " 6. (L)(April 1, 1760;CD) 1,000\n",
      " 7. COOKSVILLE,/Wisn.(185-;30;PAID/3[C];Black) 25\n",
      " 8. Andover 1841,1849-50s 15\n",
      " 9. *Cookeâ€™s Store 1852 --\n",
      "10. Orange C.H. 1801-32,1852 40\n",
      "11. Lyme Bridge 1849 250\n",
      "12. Church Hill 1846 75\n",
      "13. AKRON,O(August 2, 1833;DL box-34x16,MD;Black) 400\n",
      "14. Bucksport 1852-55,1860 150\n",
      "15. Whitneyville 1845-54 20\n"
     ]
    }
   ],
   "source": [
    "# Sample some raw data entries to see the format\n",
    "print(\"Sample txtRawStateData entries:\")\n",
    "print(\"=\"*70)\n",
    "samples = approved[approved['txtRawStateData'].notna() & \n",
    "                   (approved['txtRawStateData'] != 'NULL')]['txtRawStateData'].sample(15, random_state=42)\n",
    "for i, s in enumerate(samples, 1):\n",
    "    print(f\"{i:2}. {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Field Population Analysis\n",
    "\n",
    "**Purpose**: Understand which parsed fields are actually populated. Low population rates suggest fields that might not justify dedicated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_population_report(df, exclude_system_cols=True):\n",
    "    \"\"\"\n",
    "    Generate population rates for all fields.\n",
    "    Helps identify which fields are essential vs sparsely used.\n",
    "    \"\"\"\n",
    "    system_cols = ['dtEntered', 'dtUpdated', 'ynActive', 'ynDeleted', 'nOrder']\n",
    "    \n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        if exclude_system_cols and col in system_cols:\n",
    "            continue\n",
    "            \n",
    "        non_null = df[df[col].notna()]\n",
    "        \n",
    "        # Also exclude 'NULL' strings and empty strings\n",
    "        if df[col].dtype == 'object':\n",
    "            non_null = non_null[\n",
    "                (non_null[col] != 'NULL') & \n",
    "                (non_null[col] != '') &\n",
    "                (non_null[col] != 'n/a') &\n",
    "                (non_null[col] != '--')\n",
    "            ]\n",
    "        \n",
    "        results.append({\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'populated': len(non_null),\n",
    "            'population_rate': len(non_null) / len(df) * 100,\n",
    "            'unique_values': non_null[col].nunique() if len(non_null) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    report = pd.DataFrame(results).sort_values('population_rate', ascending=False)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essential_vs_sparse_fields(df, threshold=50):\n",
    "    \"\"\"\n",
    "    Categorize fields into essential (>threshold%) vs sparse (<threshold%).\n",
    "    \"\"\"\n",
    "    report = field_population_report(df)\n",
    "    \n",
    "    essential = report[report['population_rate'] >= threshold]\n",
    "    sparse = report[report['population_rate'] < threshold]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ESSENTIAL FIELDS (>{threshold}% populated)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(essential[['column', 'population_rate', 'unique_values']])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SPARSE FIELDS (<{threshold}% populated)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(sparse[['column', 'population_rate', 'unique_values']])\n",
    "    \n",
    "    return essential, sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ESSENTIAL FIELDS (>30% populated)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>population_rate</th>\n",
       "      <th>unique_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nRawStateDataID</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>43068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nRawStateDataID_parent</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>32844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ynForReview</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nImageCount</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ynBackstamp</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ynManuscript</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>approve_status</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nStateID</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>txtTownPostmark</td>\n",
       "      <td>99.969815</td>\n",
       "      <td>27883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>txtTown</td>\n",
       "      <td>99.923377</td>\n",
       "      <td>13896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>txtPostmark</td>\n",
       "      <td>99.185010</td>\n",
       "      <td>26710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txtRawStateData</td>\n",
       "      <td>99.078202</td>\n",
       "      <td>39706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txtRawStateDataTemp</td>\n",
       "      <td>99.078202</td>\n",
       "      <td>30668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>txtDatesSeen</td>\n",
       "      <td>94.787313</td>\n",
       "      <td>9305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>txtValue</td>\n",
       "      <td>94.675861</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>txtColors</td>\n",
       "      <td>53.900808</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>txtSizes</td>\n",
       "      <td>49.359153</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>txtRatesText</td>\n",
       "      <td>31.840346</td>\n",
       "      <td>3384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    column  population_rate  unique_values\n",
       "0          nRawStateDataID       100.000000          43068\n",
       "1   nRawStateDataID_parent       100.000000          32844\n",
       "30             ynForReview       100.000000              2\n",
       "29             nImageCount       100.000000             14\n",
       "27             ynBackstamp       100.000000              2\n",
       "26            ynManuscript       100.000000              2\n",
       "31          approve_status       100.000000              1\n",
       "2                 nStateID       100.000000             55\n",
       "13         txtTownPostmark        99.969815          27883\n",
       "12                 txtTown        99.923377          13896\n",
       "5              txtPostmark        99.185010          26710\n",
       "3          txtRawStateData        99.078202          39706\n",
       "4      txtRawStateDataTemp        99.078202          30668\n",
       "6             txtDatesSeen        94.787313           9305\n",
       "11                txtValue        94.675861            312\n",
       "8                txtColors        53.900808            360\n",
       "7                 txtSizes        49.359153           3025\n",
       "10            txtRatesText        31.840346           3384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SPARSE FIELDS (<30% populated)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>population_rate</th>\n",
       "      <th>unique_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>txtDefaultImage</td>\n",
       "      <td>14.895050</td>\n",
       "      <td>6415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>txtTownmarkColor</td>\n",
       "      <td>8.646791</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nEarliestUseYear</td>\n",
       "      <td>8.365840</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>txtTownmarkLettering</td>\n",
       "      <td>7.451008</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nLatestUseYear</td>\n",
       "      <td>5.523823</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nWidth</td>\n",
       "      <td>4.295533</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>txtTownmarkShape</td>\n",
       "      <td>2.848983</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>txtRates</td>\n",
       "      <td>2.809511</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nEarliestUseDay</td>\n",
       "      <td>2.807189</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>txtOther</td>\n",
       "      <td>2.326553</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nLatestUseDay</td>\n",
       "      <td>1.188818</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nHeight</td>\n",
       "      <td>1.063435</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>txtTownmarkDateFormat</td>\n",
       "      <td>0.578155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>txtTownmarkFraming</td>\n",
       "      <td>0.371506</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   column  population_rate  unique_values\n",
       "28        txtDefaultImage        14.895050           6415\n",
       "18       txtTownmarkColor         8.646791             93\n",
       "23       nEarliestUseYear         8.365840            126\n",
       "15   txtTownmarkLettering         7.451008              4\n",
       "25         nLatestUseYear         5.523823            121\n",
       "19                 nWidth         4.295533             70\n",
       "14       txtTownmarkShape         2.848983             24\n",
       "9                txtRates         2.809511            606\n",
       "22        nEarliestUseDay         2.807189             31\n",
       "21               txtOther         2.326553            427\n",
       "24          nLatestUseDay         1.188818             32\n",
       "20                nHeight         1.063435             48\n",
       "16  txtTownmarkDateFormat         0.578155             10\n",
       "17     txtTownmarkFraming         0.371506              6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run field population analysis\n",
    "essential, sparse = essential_vs_sparse_fields(approved, threshold=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Normalization Trade-off Analysis\n",
    "\n",
    "**Purpose**: Evaluate the cost/benefit of different normalization approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preservation_analysis(df, text_col, parsed_col):\n",
    "    \"\"\"\n",
    "    Compare original text field to its parsed equivalent.\n",
    "    Determines if parsing loses information that users need.\n",
    "    \n",
    "    Can the parsed field REPLACE the original text?\n",
    "    Or does the original text contain nuances that must be preserved?\n",
    "    \"\"\"\n",
    "    both_populated = df[\n",
    "        df[text_col].notna() & (df[text_col] != 'NULL') &\n",
    "        df[parsed_col].notna() & (df[parsed_col] != 'NULL') & (df[parsed_col] != 'n/a')\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nText Preservation Analysis: {text_col} -> {parsed_col}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Records with both fields populated: {len(both_populated):,}\")\n",
    "    \n",
    "    # Sample comparisons\n",
    "    print(\"\\nSample comparisons (original -> parsed):\")\n",
    "    sample = both_populated.sample(min(10, len(both_populated)), random_state=42)\n",
    "    for _, row in sample.iterrows():\n",
    "        orig = str(row[text_col])[:60]\n",
    "        print(f\"  '{orig}...' -> '{row[parsed_col]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_justification_report(df, field, lookup_values):\n",
    "    \"\"\"\n",
    "    Check if existing lookup table values provide coverage for the actual data.\n",
    "    \"\"\"\n",
    "    actual_values = df[df[field].notna() & (df[field] != 'NULL')][field].unique()\n",
    "    \n",
    "    covered = set(actual_values) & set(lookup_values)\n",
    "    uncovered = set(actual_values) - set(lookup_values)\n",
    "    unused_lookups = set(lookup_values) - set(actual_values)\n",
    "    \n",
    "    print(f\"\\nLookup Justification: {field}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Actual unique values: {len(actual_values)}\")\n",
    "    print(f\"Lookup table values: {len(lookup_values)}\")\n",
    "    print(f\"Covered by lookup: {len(covered)}\")\n",
    "    print(f\"Uncovered (needs adding): {len(uncovered)}\")\n",
    "    print(f\"Unused lookups: {len(unused_lookups)}\")\n",
    "    \n",
    "    if uncovered:\n",
    "        print(f\"\\nUncovered values:\")\n",
    "        for v in list(uncovered)[:20]:\n",
    "            print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Preservation Analysis: txtRawStateData -> txtTownmarkShape\n",
      "============================================================\n",
      "Records with both fields populated: 1,079\n",
      "\n",
      "Sample comparisons (original -> parsed):\n",
      "  'ERIE,ALA.(1846-47;Box-37x21;[ms date in box below town];V,PA...' -> 'Box'\n",
      "  'WILMN D.(1799-1813;27;PAID,FREE[box];Black,Red) 75...' -> 'Straight line'\n",
      "  '+MOUND CITY/K.T.(Feb. 21, --;C-35;Black) 400...' -> 'Circle'\n",
      "  'WASHINGTON CITY/D.C.(1857-58; 32.5,YMDD;PAID,3;Black) 25...' -> 'Straight line'\n",
      "  'NUEVO MEXICO.(E)(June 3, 1800;SL-51x4 letters slanting;3[ms]...' -> 'Straight line'\n",
      "  'Same(1864;DC-29,YMDD;Black) 30...' -> 'Double Circle'\n",
      "  'Same UNPAID(1873;21;--;Black) 350...' -> 'Straight line'\n",
      "  'PAID/AT/SAN JUAN PORTO RICO(1844-64;25[crowned];Black,Red) 2...' -> 'Double Circle'\n",
      "  'VIEQUES(1876-77;34x5;Black) 200...' -> 'Straight line'\n",
      "  'SIOUX FALLS CITY/D.T.(E)(Aug. 15, 1859;oval--;Black) --...' -> 'Oval'\n"
     ]
    }
   ],
   "source": [
    "# Compare raw data to parsed fields\n",
    "text_preservation_analysis(approved, 'txtRawStateData', 'txtTownmarkShape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lookup Justification: txtTownmarkShape\n",
      "============================================================\n",
      "Actual unique values: 24\n",
      "Lookup table values: 12\n",
      "Covered by lookup: 12\n",
      "Uncovered (needs adding): 12\n",
      "Unused lookups: 0\n",
      "\n",
      "Uncovered values:\n",
      "  - Fancy Oval\n",
      "  - Fancy Box\n",
      "  - Straight Line\n",
      "  - Framed Arc\n",
      "  - Double Line Oval\n",
      "  - Straight line - 3 line\n",
      "  - Shell Design\n",
      "  - Double Lined Box\n",
      "  - Straight line - 2 line\n",
      "  - Dotted Oval\n",
      "  - Dashed Circle\n",
      "  - Dotted Circle\n"
     ]
    }
   ],
   "source": [
    "# Example: Check lookup table coverage for shapes\n",
    "known_shapes = [\n",
    "    'Straight line', 'Circle', 'Double Circle', 'Oval', 'Arc',\n",
    "    'Double Line Circle', 'Double Oval', 'Fancy/morticed', 'Box',\n",
    "    'Octagon', 'Pictoral', 'Tombstone'\n",
    "]\n",
    "\n",
    "lookup_justification_report(approved, 'txtTownmarkShape', known_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Philatelic-Specific Analysis\n",
    "\n",
    "**Purpose**: Domain-specific explorations for postal history cataloging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def town_name_variations(df, town_col='txtTown', postmark_col='txtTownPostmark'):\n",
    "    \"\"\"\n",
    "    Analyze relationship between town names and postmark text.\n",
    "    \n",
    "    The postmark text on the cover may differ\n",
    "    from the normalized town name (abbreviations, historical spellings).\n",
    "    Both need to be preserved for different purposes:\n",
    "    - Normalized town: for filtering, grouping, geographic lookup\n",
    "    - Postmark text: for exact matching, historical accuracy\n",
    "    \"\"\"\n",
    "    both_populated = df[\n",
    "        df[town_col].notna() & (df[town_col] != 'NULL') &\n",
    "        df[postmark_col].notna() & (df[postmark_col] != 'NULL')\n",
    "    ]\n",
    "    \n",
    "    # Find cases where they differ\n",
    "    different = both_populated[\n",
    "        both_populated[town_col].str.lower() != both_populated[postmark_col].str.lower()\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTown Name vs Postmark Text Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Records with both: {len(both_populated):,}\")\n",
    "    print(f\"Records where they differ: {len(different):,} ({len(different)/len(both_populated)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nSamples where town name aligns with postmark:\")\n",
    "    sample = different.sample(min(15, len(different)), random_state=42)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  Town: '{row[town_col]}' | Postmark: '{row[postmark_col]}'\")\n",
    "    \n",
    "    return different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_format_patterns(df, dates_col='txtDatesSeen'):\n",
    "    \"\"\"\n",
    "    Analyze date format variations in the catalog.\n",
    "    \n",
    "    This helps determine:\n",
    "    - Whether dates can be reliably parsed into structured fields\n",
    "    - What date formats exist (e.g., \"May 21, 1772\", \"1771\", \"dateline April 24, 1767\")\n",
    "    \"\"\"\n",
    "    non_null = df[df[dates_col].notna() & (df[dates_col] != 'NULL')][dates_col]\n",
    "    \n",
    "    patterns = Counter()\n",
    "    for date_str in non_null:\n",
    "        # Classify pattern\n",
    "        if re.match(r'^\\d{4}$', str(date_str)):\n",
    "            patterns['year_only'] += 1\n",
    "        elif re.match(r'^[A-Z][a-z]+\\.?\\s+\\d{4}$', str(date_str)):\n",
    "            patterns['month_year'] += 1\n",
    "        elif re.match(r'^[A-Z][a-z]+\\.?\\s+\\d{1,2},?\\s+\\d{4}$', str(date_str)):\n",
    "            patterns['full_date'] += 1\n",
    "        elif '-' in str(date_str) or ' to ' in str(date_str).lower():\n",
    "            patterns['date_range'] += 1\n",
    "        else:\n",
    "            patterns['other'] += 1\n",
    "    \n",
    "    print(f\"\\nDate Format Patterns in {dates_col}\")\n",
    "    print(\"=\"*60)\n",
    "    total = len(non_null)\n",
    "    for pattern, count in patterns.most_common():\n",
    "        print(f\"{pattern:25} {count:6,} ({count/total*100:5.1f}%)\")\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_vocabulary_analysis(df, color_col='txtTownmarkColor'):\n",
    "    \"\"\"\n",
    "    Analyze color values to determine if controlled vocabulary is appropriate.\n",
    "    \n",
    "    Colors in postal markings are often complex:\n",
    "    - Basic: Black, Red, Blue, Brown\n",
    "    - Compound: Black,Red (multiple colors on one marking)\n",
    "    - Qualified: Light Blue, Dark Green\n",
    "    \"\"\"\n",
    "    non_null = df[df[color_col].notna() & (df[color_col] != 'NULL') & (df[color_col] != 'n/a')]\n",
    "    \n",
    "    colors = non_null[color_col].value_counts()\n",
    "    \n",
    "    print(f\"\\nColor Vocabulary Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Unique color values: {len(colors):,}\")\n",
    "    print(f\"\\nTop 30 color values:\")\n",
    "    print(colors.head(30).to_string())\n",
    "    \n",
    "    # Check for compound colors\n",
    "    compound = non_null[non_null[color_col].str.contains(',', na=False)]\n",
    "    print(f\"\\nCompound colors (contain comma): {len(compound):,}\")\n",
    "    print(compound[color_col].value_counts().head(10).to_string())\n",
    "    \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_pattern_analysis(df, sizes_col='txtSizes'):\n",
    "    \"\"\"\n",
    "    Analyze size specification patterns.\n",
    "    \n",
    "    Common formats:\n",
    "    - SL-50x3 (Straight Line, 50mm wide x 3mm tall)\n",
    "    - C-28 (Circle, 28mm diameter)\n",
    "    - Ms (Manuscript - no standard size)\n",
    "    - O-30x36 (Oval dimensions)\n",
    "    \"\"\"\n",
    "    non_null = df[df[sizes_col].notna() & (df[sizes_col] != 'NULL') & (df[sizes_col] != '')]\n",
    "    sizes = non_null[sizes_col]\n",
    "    \n",
    "    print(f\"\\nSize Specification Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Populated records: {len(sizes):,}\")\n",
    "    print(f\"Unique patterns: {sizes.nunique():,}\")\n",
    "    \n",
    "    # Categorize patterns\n",
    "    categories = Counter()\n",
    "    for s in sizes:\n",
    "        s = str(s)\n",
    "        if re.match(r'^SL-\\d+x[\\d.]+$', s):\n",
    "            categories['SL-WxH (standard straight line)'] += 1\n",
    "        elif re.match(r'^C-\\d+$', s):\n",
    "            categories['C-D (standard circle)'] += 1\n",
    "        elif 'Ms' in s:\n",
    "            categories['Ms (manuscript)'] += 1\n",
    "        elif 'SL' in s:\n",
    "            categories['SL with extras'] += 1\n",
    "        elif 'C-' in s:\n",
    "            categories['Circle with extras'] += 1\n",
    "        elif re.match(r'^[\\d.]+x[\\d.]+$', s):\n",
    "            categories['WxH numeric only'] += 1\n",
    "        elif re.match(r'^\\d+$', s):\n",
    "            categories['Single number (diameter)'] += 1\n",
    "        else:\n",
    "            categories['other'] += 1\n",
    "    \n",
    "    print(\"\\nSize format categories:\")\n",
    "    for cat, count in sorted(categories.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {cat:40} {count:6,} ({count/len(sizes)*100:5.1f}%)\")\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Town Name vs Postmark Text Analysis\n",
      "============================================================\n",
      "Records with both: 43,023\n",
      "Records where they differ: 25,407 (59.1%)\n",
      "\n",
      "Samples where town name aligns with postmark:\n",
      "  Town: 'Quakertown' | Postmark: 'QUAKERTOWN/P(â€œPâ€ in center)'\n",
      "  Town: 'Baltimore' | Postmark: 'BALTIMORE/MD'\n",
      "  Town: 'Bentonsport' | Postmark: 'Bentons Port I.T.'\n",
      "  Town: 'Charlestown' | Postmark: 'CHARLES/TOWN(backstamp)'\n",
      "  Town: 'Fort Mitchell' | Postmark: 'FORT MITCHELL AL'\n",
      "  Town: 'Urbana' | Postmark: 'URBANA O.'\n",
      "  Town: 'Atalissa' | Postmark: 'ATALISSA/IOWA'\n",
      "  Town: 'Stevens Point' | Postmark: 'STEVENS POINT/Wis'\n",
      "  Town: 'Milbury' | Postmark: 'MILBURY/Ms.'\n",
      "  Town: 'Belvidere' | Postmark: 'BELVIDERE/lll.'\n",
      "  Town: 'St. Augustine' | Postmark: 'ST.AUGUSTINE/Fl.T.'\n",
      "  Town: 'Uxbridge' | Postmark: 'UXBRIDGE/MS.'\n",
      "  Town: 'Solon' | Postmark: 'SOLON/O.'\n",
      "  Town: 'Jefferson' | Postmark: 'JEFFERSON/Ga.'\n",
      "  Town: 'Palmyra' | Postmark: 'PALMYRA/N.Y.'\n"
     ]
    }
   ],
   "source": [
    "# Run philatelic-specific analyses\n",
    "town_variations = town_name_variations(approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date Format Patterns in txtDatesSeen\n",
      "============================================================\n",
      "date_range                17,773 ( 43.5%)\n",
      "year_only                 14,058 ( 34.4%)\n",
      "full_date                  4,608 ( 11.3%)\n",
      "other                      4,311 ( 10.6%)\n",
      "month_year                    73 (  0.2%)\n"
     ]
    }
   ],
   "source": [
    "date_patterns = date_format_patterns(approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Color Vocabulary Analysis\n",
      "============================================================\n",
      "Unique color values: 93\n",
      "\n",
      "Top 30 color values:\n",
      "txtTownmarkColor\n",
      "Black                          2283\n",
      "Red                             512\n",
      "Blue                            280\n",
      "Black,Red                       169\n",
      "Green                            95\n",
      "Black,Blue                       62\n",
      "Black,Blue,Red                   57\n",
      "Blue,Red                         52\n",
      "Brown                            37\n",
      "Black,Brown                      15\n",
      "Red brown                        14\n",
      "Magenta                          10\n",
      "Brown,Red                         8\n",
      "Red orange                        8\n",
      "Orange                            7\n",
      "Red,Blue                          7\n",
      "Blue green                        5\n",
      "Black,Brown,Red                   4\n",
      "Black,Red,Blue                    4\n",
      "Black brown                       4\n",
      "n/a,Black                         4\n",
      "n/a,Red                           3\n",
      "Orange,Red                        3\n",
      "Black,Blue,Brown,Red              3\n",
      "Brownish red                      3\n",
      "Olive green                       2\n",
      "Blue,Orange,Red                   2\n",
      "Brown,Brownish,Brownish red       2\n",
      "Black,Blue,Green,Red              2\n",
      "Black brown,Red                   2\n",
      "\n",
      "Compound colors (contain comma): 454\n",
      "txtTownmarkColor\n",
      "Black,Red          169\n",
      "Black,Blue          62\n",
      "Black,Blue,Red      57\n",
      "Blue,Red            52\n",
      "Black,Brown         15\n",
      "Brown,Red            8\n",
      "Red,Blue             7\n",
      "Black,Red,Blue       4\n",
      "Black,Brown,Red      4\n",
      "n/a,Black            4\n"
     ]
    }
   ],
   "source": [
    "color_vocab = color_vocabulary_analysis(approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size Specification Analysis\n",
      "============================================================\n",
      "Populated records: 21,258\n",
      "Unique patterns: 3,025\n",
      "\n",
      "Size format categories:\n",
      "  Single number (diameter)                 11,850 ( 55.7%)\n",
      "  other                                     3,142 ( 14.8%)\n",
      "  Ms (manuscript)                           2,492 ( 11.7%)\n",
      "  SL with extras                            1,246 (  5.9%)\n",
      "  Circle with extras                        1,246 (  5.9%)\n",
      "  SL-WxH (standard straight line)             735 (  3.5%)\n",
      "  C-D (standard circle)                       394 (  1.9%)\n",
      "  WxH numeric only                            153 (  0.7%)\n"
     ]
    }
   ],
   "source": [
    "size_patterns = size_pattern_analysis(approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Raw vs Parsed Field Comparison\n",
    "\n",
    "Examine whether parsed fields capture all information from the raw source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Sample records showing raw data alongside parsed fields\\nsample = approved[\\n    approved[\\'txtRawStateData\\'].notna() & \\n    (approved[\\'txtTownPostmark\\'].notna() & approved[\\'txtPostmark\\'].notna() & approved[\\'txtTown\\'].notna()) |\\n    (approved[\\'txtRatesText\\'].notna() & approved[\\'txtRates\\'].notna()) |\\n    (approved[\\'txtTownmarkColor\\'].notna() & approved[\\'txtColors\\'].notna())\\n].sample(10, random_state=42)\\n\\nprint(\"RAW DATA vs PARSED FIELDS COMPARISON\")\\nprint(\"=\"*70)\\n\\nfor idx, row in sample.iterrows():\\n    print(f\"RAW: {row[\\'txtRawStateDataTemp\\']}\")\\n    print(f\"  Town Postmark Text: {row[\\'txtTownPostmark\\']} | Postmark Text: {row[\\'txtPostmark\\']} | Town Text: {row[\\'txtTown\\']}\")\\n    print(f\"  Rates Text: {row[\\'txtRatesText\\']} | Rates: {row[\\'txtRates\\']}\")\\n    print(f\"  Townmark Color Text: {row[\\'txtTownmarkColor\\']} | Color Text: {row[\\'txtColors\\']}\")\\n    print(f\"  Other (Notes): {row[\\'txtOther\\']}\")\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Uncomment this section if you're running against the original data, as some of these fields end up being removed\n",
    "#in the transformer as a result of this analysis\n",
    "\n",
    "\"\"\"\n",
    "# Sample records showing raw data alongside parsed fields\n",
    "sample = approved[\n",
    "    approved['txtRawStateData'].notna() & \n",
    "    (approved['txtTownPostmark'].notna() & approved['txtPostmark'].notna() & approved['txtTown'].notna()) |\n",
    "    (approved['txtRatesText'].notna() & approved['txtRates'].notna()) |\n",
    "    (approved['txtTownmarkColor'].notna() & approved['txtColors'].notna())\n",
    "].sample(10, random_state=42)\n",
    "\n",
    "print(\"RAW DATA vs PARSED FIELDS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"RAW: {row['txtRawStateDataTemp']}\")\n",
    "    print(f\"  Town Postmark Text: {row['txtTownPostmark']} | Postmark Text: {row['txtPostmark']} | Town Text: {row['txtTown']}\")\n",
    "    print(f\"  Rates Text: {row['txtRatesText']} | Rates: {row['txtRates']}\")\n",
    "    print(f\"  Townmark Color Text: {row['txtTownmarkColor']} | Color Text: {row['txtColors']}\")\n",
    "    print(f\"  Other (Notes): {row['txtOther']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.1. Geographic Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State counts by ID:\n",
      "nStateID\n",
      "32    5672\n",
      "21    3847\n",
      "38    2792\n",
      "35    2747\n",
      "46    2115\n",
      "19    1730\n",
      "13    1592\n",
      "22    1452\n",
      "49    1374\n",
      "7     1332\n",
      "45    1268\n",
      "14    1111\n",
      "33    1106\n",
      "29     991\n",
      "24     982\n",
      "1      917\n",
      "10     891\n",
      "15     863\n",
      "17     861\n",
      "30     809\n",
      "20     787\n",
      "42     771\n",
      "25     750\n",
      "5      677\n",
      "43     667\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# State distribution\n",
    "state_counts = approved['nStateID'].value_counts().head(25)\n",
    "\n",
    "# Try to load state names\n",
    "try:\n",
    "    states_df = pd.read_csv('./wip/tblStates.csv')\n",
    "    state_map = dict(zip(states_df['nStateID'], states_df['txtState']))\n",
    "    \n",
    "    print(\"Records by State (Top 25)\")\n",
    "    print(\"=\"*50)\n",
    "    for state_id, count in state_counts.items():\n",
    "        name = state_map.get(state_id, f'Unknown ({state_id})')\n",
    "        print(f\"  {name:25} {count:6,}\")\n",
    "except:\n",
    "    print(\"State counts by ID:\")\n",
    "    print(state_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.2. Color Handling Ambiguity Analysis\n",
    "\n",
    "### The Documentation Gap\n",
    "\n",
    "A critical question for database normalization is: **When a catalog entry lists multiple colors (e.g., \"Black,Red\"), does this represent:**\n",
    "\n",
    "1. **One postmark device** observed in multiple ink colors over its period of use?\n",
    "2. **Multiple separate observations** that should be distinct records?\n",
    "3. **A single cover** with multiple ink colors on the same marking?\n",
    "\n",
    "### What the ASCC Header Actually Says\n",
    "\n",
    "The **only** explicit text about colors in the ASCC catalog introduction (Page xv, Section 5: \"COLOR OF MARKINGS\"):\n",
    "\n",
    "> *\"Manuscript markings are commonly found applied in black ink. This catalog makes no distinction in scarcity and value for manuscript markings applied in colors other than black except in the case of Territorial and Colonial markings.*\n",
    ">\n",
    "> *Handstamped markings are commonly found applied in black, blue and red, and generally no distinction is made in evaluating markings in these colors. Handstamped markings applied in green, purple, magenta, yellow, brown and orange are considerably scarcer and listings in this catalog often reflect increased valuations for markings known to exist in these colors. Red markings sometimes turn brownish with age.\"*\n",
    "\n",
    "**Critically absent from the documentation:**\n",
    "- What comma-separated colors in a listing mean\n",
    "- Whether multiple colors represent one device or separate observations\n",
    "- Any formatting conventions for color notation\n",
    "- How to interpret compound colors like \"Brown black\" vs \"Black,Brown\"\n",
    "\n",
    "This section analyzes what the **data structure itself** implies about color semantics, acknowledging that these are inferences, not documented conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_color_field_structure(df, color_col='txtColors', alt_color_col='txtTownmarkColor'):\n",
    "    \"\"\"\n",
    "    - Are colors stored as single values or comma-separated lists?\n",
    "    - What delimiters are used?\n",
    "    - How do the two color fields relate?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Analyze primary color field\n",
    "    for col in [color_col, alt_color_col]:\n",
    "        if col not in df.columns:\n",
    "            print(f\"\\nColumn {col} not found in dataframe\")\n",
    "            continue\n",
    "            \n",
    "        non_null = df[df[col].notna() & (df[col] != 'NULL') & (df[col] != 'n/a') & (df[col] != '')]\n",
    "        \n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        print(f\"Populated records: {len(non_null):,}\")\n",
    "        \n",
    "        # Count entries with multiple colors (comma-separated)\n",
    "        has_comma = non_null[non_null[col].str.contains(',', na=False)]\n",
    "        print(f\"Entries with comma (potential multi-color): {len(has_comma):,} ({len(has_comma)/len(non_null)*100:.1f}%)\")\n",
    "        \n",
    "        # Count entries with spaces that might indicate compound colors\n",
    "        has_space = non_null[non_null[col].str.contains(' ', na=False)]\n",
    "        print(f\"Entries with space: {len(has_space):,} ({len(has_space)/len(non_null)*100:.1f}%)\")\n",
    "        \n",
    "        # Sample multi-color entries\n",
    "        if len(has_comma) > 0:\n",
    "            print(f\"\\nSample multi-color values (first 15):\")\n",
    "            for val in has_comma[col].value_counts().head(15).index:\n",
    "                count = (non_null[col] == val).sum()\n",
    "                print(f\"  '{val}' ({count:,} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- txtColors ---\n",
      "Populated records: 23,214\n",
      "Entries with comma (potential multi-color): 5,356 (23.1%)\n",
      "Entries with space: 470 (2.0%)\n",
      "\n",
      "Sample multi-color values (first 15):\n",
      "  'Black,Red' (2,313 records)\n",
      "  'Black,Blue,Red' (883 records)\n",
      "  'Black,Blue' (688 records)\n",
      "  'Blue,Red' (556 records)\n",
      "  'Orange,Red' (65 records)\n",
      "  'Black,Brown' (60 records)\n",
      "  'Brown,Red' (53 records)\n",
      "  'Black,Brown,Red' (49 records)\n",
      "  'Black,Orange,Red' (37 records)\n",
      "  'Black,Blue,Brown,Red' (36 records)\n",
      "  'Black,Blue,Orange,Red' (29 records)\n",
      "  'Blue,Orange,Red' (26 records)\n",
      "  'Black,Blue,Green,Red' (26 records)\n",
      "  'Black,Green' (19 records)\n",
      "  'Black,Green,Red' (19 records)\n",
      "\n",
      "--- txtTownmarkColor ---\n",
      "Populated records: 3,724\n",
      "Entries with comma (potential multi-color): 454 (12.2%)\n",
      "Entries with space: 75 (2.0%)\n",
      "\n",
      "Sample multi-color values (first 15):\n",
      "  'Black,Red' (169 records)\n",
      "  'Black,Blue' (62 records)\n",
      "  'Black,Blue,Red' (57 records)\n",
      "  'Blue,Red' (52 records)\n",
      "  'Black,Brown' (15 records)\n",
      "  'Brown,Red' (8 records)\n",
      "  'Red,Blue' (7 records)\n",
      "  'Black,Red,Blue' (4 records)\n",
      "  'Black,Brown,Red' (4 records)\n",
      "  'n/a,Black' (4 records)\n",
      "  'Orange,Red' (3 records)\n",
      "  'Black,Blue,Brown,Red' (3 records)\n",
      "  'n/a,Red' (3 records)\n",
      "  'Black brown,Red' (2 records)\n",
      "  'Black,Green' (2 records)\n"
     ]
    }
   ],
   "source": [
    "# Run color field structure analysis\n",
    "analyze_color_field_structure(approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_date_correlation_analysis(df, color_col='txtColors', dates_col='txtDatesSeen'):\n",
    "    \"\"\"\n",
    "    Analyze correlation between multi-color entries and date patterns.\n",
    "    \n",
    "    HYPOTHESIS: If multi-color entries represent one device observed over time,\n",
    "    they should MORE OFTEN have date RANGES than single-color entries.\n",
    "    \n",
    "    This would suggest: \"Black,Red\" = one device, observed in black ink on some dates,\n",
    "    red ink on others, across a span of years.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to records with both color and date info\n",
    "    has_both = df[\n",
    "        df[color_col].notna() & (df[color_col] != 'NULL') & (df[color_col] != '') &\n",
    "        df[dates_col].notna() & (df[dates_col] != 'NULL') & (df[dates_col] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nRecords with both color and date: {len(has_both):,}\")\n",
    "    \n",
    "    # Classify color entries\n",
    "    has_both['is_multi_color'] = has_both[color_col].str.contains(',', na=False)\n",
    "    \n",
    "    # Classify date entries (range vs single date)\n",
    "    def classify_date(date_str):\n",
    "        if pd.isna(date_str) or date_str in ['NULL', '', 'n/a']:\n",
    "            return 'missing'\n",
    "        date_str = str(date_str)\n",
    "        if '-' in date_str and not date_str.startswith('-'):\n",
    "            return 'date_range'\n",
    "        elif re.match(r'^\\d{4}$', date_str):\n",
    "            return 'year_only'\n",
    "        elif re.match(r'^[A-Za-z]+\\.?\\s+\\d{1,2},?\\s+\\d{4}$', date_str):\n",
    "            return 'specific_date'\n",
    "        elif re.match(r'^[A-Za-z]+\\.?\\s+\\d{4}$', date_str):\n",
    "            return 'month_year'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    has_both['date_type'] = has_both[dates_col].apply(classify_date)\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    print(\"\\n--- Date Pattern by Color Type ---\")\n",
    "    \n",
    "    single_color = has_both[~has_both['is_multi_color']]\n",
    "    multi_color = has_both[has_both['is_multi_color']]\n",
    "    \n",
    "    print(f\"\\nSingle-color entries: {len(single_color):,}\")\n",
    "    single_dist = single_color['date_type'].value_counts()\n",
    "    for dt, count in single_dist.items():\n",
    "        print(f\"  {dt:20} {count:6,} ({count/len(single_color)*100:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMulti-color entries: {len(multi_color):,}\")\n",
    "    if len(multi_color) > 0:\n",
    "        multi_dist = multi_color['date_type'].value_counts()\n",
    "        for dt, count in multi_dist.items():\n",
    "            print(f\"  {dt:20} {count:6,} ({count/len(multi_color)*100:5.1f}%)\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    single_range_pct = (single_color['date_type'] == 'date_range').mean() * 100\n",
    "    multi_range_pct = (multi_color['date_type'] == 'date_range').mean() * 100 if len(multi_color) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- KEY FINDING ---\")\n",
    "    print(f\"Single-color entries with date ranges: {single_range_pct:.1f}%\")\n",
    "    print(f\"Multi-color entries with date ranges:  {multi_range_pct:.1f}%\")\n",
    "    \n",
    "    if multi_range_pct > single_range_pct:\n",
    "        print(f\"\\n-> Multi-color entries are {multi_range_pct/single_range_pct:.1f}x MORE LIKELY to have date ranges.\")\n",
    "        print(\"  This SUPPORTS the hypothesis that multi-color = one device over time.\")\n",
    "    else:\n",
    "        print(\"\\n-> No significant difference found. Hypothesis not supported by this evidence.\")\n",
    "    \n",
    "    return has_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Records with both color and date: 21,970\n",
      "\n",
      "--- Date Pattern by Color Type ---\n",
      "\n",
      "Single-color entries: 16,709\n",
      "  year_only             6,981 ( 41.8%)\n",
      "  date_range            4,958 ( 29.7%)\n",
      "  specific_date         2,980 ( 17.8%)\n",
      "  other                 1,729 ( 10.3%)\n",
      "  month_year               61 (  0.4%)\n",
      "\n",
      "Multi-color entries: 5,261\n",
      "  date_range            4,429 ( 84.2%)\n",
      "  year_only               585 ( 11.1%)\n",
      "  other                   166 (  3.2%)\n",
      "  specific_date            81 (  1.5%)\n",
      "\n",
      "--- KEY FINDING ---\n",
      "Single-color entries with date ranges: 29.7%\n",
      "Multi-color entries with date ranges:  84.2%\n",
      "\n",
      "-> Multi-color entries are 2.8x MORE LIKELY to have date ranges.\n",
      "  This SUPPORTS the hypothesis that multi-color = one device over time.\n"
     ]
    }
   ],
   "source": [
    "# Run color-date correlation analysis\n",
    "color_date_df = color_date_correlation_analysis(approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_disambiguation_datasets(df, color_col='txtColors', dates_col='txtDatesSeen',\n",
    "                                          output_dir='./wip/out/'):\n",
    "    \"\"\"\n",
    "    Generate datasets to help manually disambiguate color interpretation.\n",
    "    \n",
    "    Produces:\n",
    "    1. multi_color_with_ranges.csv - Multi-color entries with date ranges (likely one device)\n",
    "    2. multi_color_specific_dates.csv - Multi-color with specific dates (ambiguous)\n",
    "    3. compound_colors.csv - Entries with space-separated colors (e.g., \"Brown black\")\n",
    "    4. color_vocabulary.csv - All unique color values with frequencies\n",
    "    5. color_by_shape.csv - Color distribution by postmark shape\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"GENERATING COLOR DISAMBIGUATION DATASETS\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter to records with color data\n",
    "    has_color = df[\n",
    "        df[color_col].notna() & \n",
    "        (df[color_col] != 'NULL') & \n",
    "        (df[color_col] != 'n/a') & \n",
    "        (df[color_col] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Classify entries\n",
    "    has_color['has_comma'] = has_color[color_col].str.contains(',', na=False)\n",
    "    has_color['has_space'] = has_color[color_col].str.contains(' ', na=False) & ~has_color['has_comma']\n",
    "    \n",
    "    # Date classification\n",
    "    def has_date_range(date_str):\n",
    "        if pd.isna(date_str) or str(date_str) in ['NULL', '', 'n/a']:\n",
    "            return False\n",
    "        return '-' in str(date_str) and not str(date_str).startswith('-')\n",
    "    \n",
    "    has_color['has_date_range'] = has_color[dates_col].apply(has_date_range)\n",
    "    \n",
    "    # Select columns for export\n",
    "    export_cols = [\n",
    "        'nRawStateDataID', 'txtTown', 'txtPostmark', color_col, \n",
    "        'txtTownmarkColor', dates_col, 'txtTownmarkShape', \n",
    "        'txtSizes', 'txtRawStateData'\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in has_color.columns]\n",
    "    \n",
    "    # 1. Multi-color with date ranges\n",
    "    multi_with_ranges = has_color[has_color['has_comma'] & has_color['has_date_range']][export_cols]\n",
    "    multi_with_ranges.to_csv(f\"{output_dir}/multi_color_with_ranges.csv\", index=False)\n",
    "    print(f\"\\n1. multi_color_with_ranges.csv: {len(multi_with_ranges):,} records\")\n",
    "    print(\"   -> These likely represent ONE DEVICE observed in multiple inks over time\")\n",
    "    \n",
    "    # 2. Multi-color with specific dates (ambiguous)\n",
    "    multi_specific = has_color[has_color['has_comma'] & ~has_color['has_date_range']][export_cols]\n",
    "    multi_specific.to_csv(f\"{output_dir}/multi_color_specific_dates.csv\", index=False)\n",
    "    print(f\"\\n2. multi_color_specific_dates.csv: {len(multi_specific):,} records\")\n",
    "    print(\"   -> AMBIGUOUS: Could be one device or multiple observations\")\n",
    "    \n",
    "    # 3. Compound colors (space-separated)\n",
    "    compound = has_color[has_color['has_space']][export_cols]\n",
    "    compound.to_csv(f\"{output_dir}/compound_colors.csv\", index=False)\n",
    "    print(f\"\\n3. compound_colors.csv: {len(compound):,} records\")\n",
    "    print(\"   -> Space-separated colors (e.g., 'Brown black') - meaning unclear\")\n",
    "    \n",
    "    # 4. Color vocabulary with frequencies\n",
    "    color_vocab = has_color[color_col].value_counts().reset_index()\n",
    "    color_vocab.columns = ['color_value', 'frequency']\n",
    "    color_vocab['is_multi'] = color_vocab['color_value'].str.contains(',')\n",
    "    color_vocab['has_space'] = color_vocab['color_value'].str.contains(' ') & ~color_vocab['is_multi']\n",
    "    color_vocab.to_csv(f\"{output_dir}/color_vocabulary.csv\", index=False)\n",
    "    print(f\"\\n4. color_vocabulary.csv: {len(color_vocab):,} unique color values\")\n",
    "    \n",
    "    # 5. Color by shape cross-tab\n",
    "    if 'txtTownmarkShape' in has_color.columns:\n",
    "        color_shape = has_color.groupby(['txtTownmarkShape', color_col]).size().reset_index(name='count')\n",
    "        color_shape = color_shape.sort_values('count', ascending=False)\n",
    "        color_shape.to_csv(f\"{output_dir}/color_by_shape.csv\", index=False)\n",
    "        print(f\"\\n5. color_by_shape.csv: {len(color_shape):,} shape-color combinations\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total records with color data: {len(has_color):,}\")\n",
    "    print(f\"  - Single color: {(~has_color['has_comma'] & ~has_color['has_space']).sum():,}\")\n",
    "    print(f\"  - Multi-color (comma): {has_color['has_comma'].sum():,}\")\n",
    "    print(f\"  - Compound (space): {has_color['has_space'].sum():,}\")\n",
    "    \n",
    "    return {\n",
    "        'multi_with_ranges': multi_with_ranges,\n",
    "        'multi_specific': multi_specific,\n",
    "        'compound': compound,\n",
    "        'vocabulary': color_vocab\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate color disambiguation datasets\n",
    "# Uncomment to run\n",
    "# color_datasets = extract_color_disambiguation_datasets(approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Ambiguity: Conclusions\n",
    "\n",
    "#### What the Data Structure SUGGESTS (Inferences, Not Proven):\n",
    "\n",
    "| Evidence | Finding | Implication |\n",
    "|----------|---------|-------------|\n",
    "| **txtColors field** | Stores \"Black,Red\" as a single string | Designed to keep colors together as one unit |\n",
    "| **Date correlation** | Multi-color entries more often have date ranges | Multi-color = period of use, not single observation |\n",
    "| **No normalization** | txtTownmarkColor also holds comma-separated values | Neither field was designed to extract individual colors |\n",
    "\n",
    "#### What CANNOT Be Proven From Available Documentation:\n",
    "\n",
    "- The **exact meaning** of comma-separated colors from ASCC documentation\n",
    "- Whether \"Black,Red\" means one device with multiple inks, or something else\n",
    "- What space-separated compound colors (like \"Brown black\") definitively mean\n",
    "- Whether the data entry conventions were consistent across all catalogers\n",
    "\n",
    "#### Recommended Approach for Normalization:\n",
    "\n",
    "1. **Preserve the original string** in txtColors/txtTownmarkColor as-is (authoritative source)\n",
    "2. **Create a junction table** for querying by individual colors:\n",
    "   - `PostmarkColors(postmark_id, color_id, is_primary)`\n",
    "   - Split comma-separated values when populating\n",
    "3. **Flag compound colors** (space-separated) for manual review\n",
    "4. **Accept ambiguity** - some entries may never have a definitive interpretation\n",
    "\n",
    "The strong correlation between multi-color entries and date ranges is **suggestive but not definitive**. Without explicit ASCC documentation stating the convention, any interpretation remains inference from data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Decision Framework & Analysis Summary\n",
    "\n",
    "### Normalization Decision Criteria\n",
    "\n",
    "**NORMALIZE INTO LOOKUP TABLE WHEN:**\n",
    "- Cardinality < 50 unique values\n",
    "- Top 10 values cover > 80% of records\n",
    "- Users need to FILTER/FACET by this attribute\n",
    "- Controlled vocabulary adds value (standardization)\n",
    "- Additional metadata needed (display order, descriptions)\n",
    "\n",
    "**KEEP AS TEXT FIELD WHEN:**\n",
    "- High cardinality (approaches record count)\n",
    "- Values contain nuance that categories would lose\n",
    "- Historical accuracy requires exact preservation\n",
    "- Field is rarely used for filtering (< 30% populated)\n",
    "- Forcing into categories creates \"Other\" catchall problems\n",
    "\n",
    "**DUAL APPROACH (both lookup + text) WHEN:**\n",
    "- Text appears ON the physical artifact (postmark text, rates)\n",
    "- FK for filtering/classification\n",
    "- Text field preserves exact appearance\n",
    "\n",
    "**ALWAYS PRESERVE: txtRawStateData**\n",
    "- Authoritative source text from the ASCC catalog\n",
    "- Even if perfectly parsed, keep for:\n",
    "  - Audit trail / provenance\n",
    "  - Reparsing if rules improve\n",
    "  - Edge cases that don't fit schema\n",
    "  - Scholar citation needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STRONG LOOKUP TABLE CANDIDATES:\n",
      "  - txtTownmarkShape: 24 values, 92% in top 5\n",
      "  - txtTownmarkLettering: 4 values, 100% in top 5\n",
      "  - txtTownmarkDateFormat: 10 values, 85% in top 5\n",
      "  - txtTownmarkFraming: 6 values, 99% in top 5\n",
      "  - txtTownmarkColor: 93 values, 90% in top 5\n",
      "\n",
      "PRESERVE AS TEXT (high cardinality or sparse):\n",
      "  - txtSizes: 3025 unique, 49.4% populated\n",
      "  - txtDatesSeen: 9305 unique, 94.8% populated\n",
      "  - txtColors: 360 unique, 53.9% populated\n",
      "  - txtRatesText: 3384 unique, 31.8% populated\n",
      "  - txtOther: 427 unique, 2.3% populated\n",
      "\n",
      "ALWAYS PRESERVE:\n",
      "  - txtRawStateData (authoritative source)\n",
      "  - txtPostmark (exact marking text)\n",
      "  - txtTownPostmark (differs from txtTown 59% of time)\n",
      "\n",
      "CANDIDATES FOR DEPRECATION (<30% populated):\n",
      "  - txtDefaultImage: 14.90%\n",
      "  - txtTownmarkColor: 8.65%\n",
      "  - nEarliestUseYear: 8.37%\n",
      "  - txtTownmarkLettering: 7.45%\n",
      "  - nLatestUseYear: 5.52%\n",
      "  - nWidth: 4.30%\n",
      "  - txtTownmarkShape: 2.85%\n",
      "  - txtRates: 2.81%\n",
      "  - nEarliestUseDay: 2.81%\n",
      "  - txtOther: 2.33%\n",
      "  - nLatestUseDay: 1.19%\n",
      "  - nHeight: 1.06%\n",
      "  - txtTownmarkDateFormat: 0.58%\n",
      "  - txtTownmarkFraming: 0.37%\n"
     ]
    }
   ],
   "source": [
    "# Summary recommendation generator\n",
    "def generate_normalization_summary(df):\n",
    "    \"\"\"\n",
    "    Generate a summary of normalization recommendations based on the analysis.\n",
    "    \"\"\"\n",
    "    classification_cols = [\n",
    "        'txtTownmarkShape', 'txtTownmarkLettering', 'txtTownmarkDateFormat',\n",
    "        'txtTownmarkFraming', 'txtTownmarkColor'\n",
    "    ]\n",
    "    \n",
    "    card = cardinality_analysis(df, classification_cols)\n",
    "    pop = field_population_report(df)\n",
    "    \n",
    "    print(\"\\nSTRONG LOOKUP TABLE CANDIDATES:\")\n",
    "    for _, row in card[card['normalization_candidate'] == True].iterrows():\n",
    "        print(f\"  - {row['column']}: {row['unique_values']} values, \"\n",
    "              f\"{row['top_5_concentration_pct']:.0f}% in top 5\")\n",
    "    \n",
    "    print(\"\\nPRESERVE AS TEXT (high cardinality or sparse):\")\n",
    "    text_candidates = ['txtSizes', 'txtDatesSeen', 'txtColors', 'txtRatesText', 'txtOther']\n",
    "    for col in text_candidates:\n",
    "        pop_row = pop[pop['column'] == col]\n",
    "        if len(pop_row) > 0:\n",
    "            print(f\"  - {col}: {pop_row['unique_values'].values[0]} unique, \"\n",
    "                  f\"{pop_row['population_rate'].values[0]:.1f}% populated\")\n",
    "    \n",
    "    print(\"\\nALWAYS PRESERVE:\")\n",
    "    print(\"  - txtRawStateData (authoritative source)\")\n",
    "    print(\"  - txtPostmark (exact marking text)\")\n",
    "    print(\"  - txtTownPostmark (differs from txtTown 59% of time)\")\n",
    "    \n",
    "    print(\"\\nCANDIDATES FOR DEPRECATION (<30% populated):\")\n",
    "    deprecated = pop[pop['population_rate'] < 30][['column', 'population_rate']]\n",
    "    for _, row in deprecated.iterrows():\n",
    "        print(f\"  - {row['column']}: {row['population_rate']:.2f}%\")\n",
    "\n",
    "generate_normalization_summary(approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covers-zQZJ1Gor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
